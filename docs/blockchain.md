# Digital Product Passport – Advanced Architecture and Strategy

## Advanced Cryptographic Infrastructure
To ensure both compliance and privacy, the DPP should use cutting-edge cryptography. Zero-knowledge proofs (ZKPs) can prove statements (e.g. "material X ≥ Y% recycled") without revealing sensitive data [circularise.com](https://circularise.com). For instance, zk-SNARK or zk-STARK circuits can encode certified attributes (origin, composition) so verifiers learn only “true/false,” not the underlying details [circularise.com](https://circularise.com). Popular libraries (e.g. Circom, ZoKrates for SNARKs; StarkWare tools for STARKs) or frameworks (like Aztec or Zcash primitives) can be leveraged for on-chain proofs.

### Zero-Knowledge Compliance
Implement ZK-SNARK/STARK protocols so a manufacturer can attest to regulatory facts (e.g. “substance levels below legal limits”) without exposing full supplier lists. ZKP schemes differ in trust assumptions: SNARKs (e.g. Groth16) need a setup, STARKs (e.g. from StarkWare) do not [circularise.com](https://circularise.com). The choice depends on performance and auditability.

### Scalable Revocation
Use cryptographic accumulators or the W3C Status List v2021 to revoke credentials at scale. An accumulator (as in Hyperledger Indy) assigns each issued VC a secret factor; revoking removes its factor from the public accumulator, and holders can prove non-revocation [hyperledger-indy.readthedocs.io](https://hyperledger-indy.readthedocs.io). Alternatively, follow W3C’s Status List 2021 approach: publish one compressed bitstring per issuer where each bit index corresponds to a VC. A bit=1 marks revocation, 0 = valid. This bundles millions of statuses in one list, preserving privacy while enabling fast checks [w3.org](https://w3.org).

### Selective Disclosure (VC Privacy)
Support BBS+ signature suites with JSON-LD to let holders reveal only needed attributes. The W3C’s Data Integrity BBS Cryptosuite uses BBS+ to create unlinkable derived proofs [w3.org](https://w3.org). In practice, issuers sign a full JSON-LD credential with BBS+ (on curve BLS12-381 [decentralized-id.com](https://decentralized-id.com)) and holders can selectively reveal fields (e.g. only “meets certification” without supplier ID). This JSON-LD + BBS+ pipeline meets privacy mandates: data minimization and holder-control of disclosure.

### Zero-Knowledge Proof Developer Stack
Circom – A mature, low-level DSL by Baylina/iden3 for writing R1CS circuits. It offers fine-grained control over constraints, enabling high performance via optimized WebAssembly and tools like RapidSnark [medium.com](https://medium.com) [medium.com](https://medium.com). Circom is widely used in practice (e.g. Tornado Cash, Dark Forest) and produces small proofs for on-chain verification. However, developers must manually handle non-linear constraints (e.g. the “inverse trick” for checking zero) [medium.com](https://medium.com), making it more like driving a manual-transmission car [medium.com](https://medium.com).

ZoKrates – A high-level Rust-based zk-SNARK toolbox aimed at Ethereum. It provides a simple language and compiler, plus built-in tools for circuit optimization and proof generation [linea.mirror.xyz](https://linea.mirror.xyz) [linea.mirror.xyz](https://linea.mirror.xyz). ZoKrates abstracts much of the low-level detail (for example, adding two fields is just def main(field x, field y) -> field { return x + y; }), and tightly integrates with Solidity (automatic verifier contract). Its focus is developer accessibility and rapid integration into smart contracts [linea.mirror.xyz](https://linea.mirror.xyz) [linea.mirror.xyz](https://linea.mirror.xyz). As a result, ZoKrates is “user-friendly for developers of all levels” and highly customizable [linea.mirror.xyz](https://linea.mirror.xyz).

SnarkyJS (o1js) – A TypeScript ZKP library from O(1)Labs (Mina Protocol). It enables writing ZK programs in JS/TS and provides rich cryptographic primitives. SnarkyJS “seamlessly integrates” with JavaScript toolchains and has VS Code support for completion and debugging [medium.com](https://medium.com) [medium.com](https://medium.com). It’s designed for full-stack ZK apps (including browser and Node), with many built-ins (hashes, curves, fields) and a growing ecosystem. The TypeScript interface lowers the barrier for web developers, and the library outputs proofs/verifiers compatible with Mina’s blockchain or custom chains [medium.com](https://medium.com).

Noir – A new Rust-inspired ZKP DSL by Aztec focusing on usability. It compiles to an abstract ACIR IR and can target multiple backends (PLONK, etc.). Noir’s syntax is high-level and C-like, automatically handling constraint logic under the hood. As one author notes, Circom forces developers to “play more of a cryptographer’s role,” whereas Noir lets you write simple logic (e.g. fn isZero(x: Field) -> bool { x == 0 }) without manual R1CS gymnastics [medium.com](https://medium.com). Noir includes a rich standard library (SHA-256, Pedersen, Merkle, math ops) and auto-generates Solidity verifiers. This “automatic transmission” approach saves developer effort [medium.com](https://medium.com), though potentially at some performance cost relative to hand-optimized circuits.

For real-world circuits (e.g. proving “no RoHS-banned substances”), one would encode threshold checks on chemical composition. The circuit might take hashed material data or concentration values as private inputs and enforce that each banned-substance value equals zero or below a regulatory limit. In Noir or ZoKrates this could be expressed with a high-level loop or arithmetic condition, while Circom would require combining multiplication constraints (e.g. in * out == 0) to emulate an equality check [medium.com](https://medium.com). The prover then emits a proof that all checks pass without revealing the raw values, attesting compliance privately. In practice, such circuits often output a binary pass/fail flag and may leverage commitments or Merkle proofs for data integrity. (For example, supply-chain ZKP demos show companies proving adherence to environmental standards without revealing sensitive details [meegle.com](https://meegle.com).) Circom and ZoKrates circuits can be optimized (minimize multipliers, reuse gadgets, apply built-in optimizers) to reduce proving time. ZoKrates explicitly offers optimization tools in its compiler [linea.mirror.xyz](https://linea.mirror.xyz). Circom benefits from highly-optimized backends (e.g. WASM provers, RapidSnark parallel proving) that yield impressive performance [medium.com](https://medium.com). Noir’s compiler can apply IR-level optimizations across targets. Finally, trusted-setup vs transparent trade-offs influence tooling: traditional Groth16-based pipelines require a CRS ceremony (per-circuit or universal) whereas PLONK/Sonic/Marlin use one global SRS, and zk-STARKs or Halo2/IPAs avoid any secret setup [blog.lambdaclass.com](https://blog.lambdaclass.com) [linea.mirror.xyz](https://linea.mirror.xyz). SNARKs (Groth16) give tiny proofs but need trusted setup [tlu.tarilabs.com](https://tlu.tarilabs.com), whereas transparent SNARKs/STARKs eliminate trust assumptions [linea.mirror.xyz](https://linea.mirror.xyz) at the cost of larger proofs. In choosing a stack, developers balance proof size, prover speed, and trust model, as well as ergonomics (Noir and o1js simplify development [medium.com](https://medium.com), while Circom/Zokrates may squeeze more performance at the cost of manual constraint management).

## Regulatory Interoperability and Global Standards Mapping
The DPP must align with international traceability and circular-economy standards. For supply chain events, GS1 EPCIS 2.0 is a natural fit: it tracks production, shipping, and logistics events. For example, a proof-of-concept DPP uses “EPCIS 2.0 for batch and transport event logs” [cryptokastaar.com](https://cryptokastaar.com). Likewise, ISO 22005 (feed/food chain traceability) provides a proven framework for recording each link in a product’s lineage [cryptokastaar.com](https://cryptokastaar.com). It is advisable to encode product IDs via GS1 Digital Link URIs (with GTINs) and embed them in QR/NFC carriers to meet ESPR requirements [cryptokastaar.com](https://cryptokastaar.com).

### Global Traceability Standards
Adopt EPCIS 2.0 and GS1 Digital Link to interoperate with existing supply-chain systems [cryptokastaar.com](https://cryptokastaar.com). Ensure data formats follow ISO/IEC 19944 (cloud data flows) or ISO 22005 (chain-of-custody) principles to facilitate integration. For instance, a DPP might use ISO 22005-derived schemas for product batch records, and link them into a distributed ledger. An example DPP pilot explicitly notes it “follows ISO 22005 (food chain traceability)” and uses GS1 standards to encode and share data [cryptokastaar.com](https://cryptokastaar.com).

### US Regulatory Mapping
Map DPP data fields to US chemical and safety regimes. Key attributes like substance composition or hazard info should correspond to TSCA (EPA’s Toxic Substances Control Act) inventory fields and California Prop 65 warning lists [3eco.com](https://3eco.com). For example, the DPP schema should flag any chemicals listed under TSCA or Prop 65, so US customers can automatically see compliance status. Aligning to these schemas enables U.S. OEMs and consumers to use the DPP for regulatory reporting [3eco.com](https://3eco.com).

### Harmonization with UNECE/OECD
Leverage global data models and circular-economy metrics. UNECE’s UN/CEFACT team has published JSON-LD vocabularies for DPP (traceability, circularity) to ensure cross-border semantic compatibility [jargon-sh.medium.com](https://jargon-sh.medium.com). The DPP should reuse or align with these open vocabularies to facilitate interoperability. Similarly, OECD and EU reports emphasize that DPPs must capture material composition and life-cycle data for sustainability indicators [oecd.org](https://oecd.org). Thus, use common ontologies (e.g. UN sustainability goals, OECD indicators) for reporting environmental impact, so the DPP serves both EU ESPR and global circular-economy datasets [oecd.org](https://jargon-sh.medium.com).

---

# Smart Contract Architecture and Anchoring Models

## Anchor Aggregation with Merkle Trees and ZK Proofs
The DPP platform’s smart contracts should minimize on-chain operations by aggregating credential anchors. One approach is to use Merkle tree batching: hash each credential (or update) into a Merkle tree and only publish the root on-chain [cs.purdue.edu](https://cs.purdue.edu). This method allows thousands of credentials to share one anchor, drastically reducing gas costs (e.g. anchoring each VC individually might cost ~576k gas, versus ~78k gas for a single Merkle root that covers many credentials [cs.purdue.edu](https://cs.purdue.edu)). To preserve trust, off-chain proofs can be generated – a holder provides a Merkle inclusion proof that their credential was part of the anchored root, and verifiers can validate it against the on-chain root. Additionally, zero-knowledge proofs (ZKPs) can attest compliance without exposing data. Using ZK-SNARKs or similar, an issuer can prove a product meets certain standards (e.g. contains < X% of a restricted substance) without revealing the underlying proprietary data [circularise.com](https://circularise.com). This “proof-of-compliance” via ZKP balances transparency and IP protection, enabling verifiers to trust regulatory claims while sensitive product data remains confidential [circularise.com](https://circularise.com). The smart contracts would simply verify the ZK proof on-chain (or an off-chain service would do so and anchor a certificate), avoiding large data payloads and ensuring privacy.

## Gas Optimization in Credential Workflows
Every on-chain interaction (issuance, update, revocation) must be optimized for cost, especially if scaled to millions of products. By leveraging batched operations and compact data structures, the platform avoids redundant writes. For instance, instead of writing a new state for each credential update, the contract could store only a reference to a rolling Merkle root or use event logs for off-chain aggregation. Techniques like stateless proofs (where the chain stores a constant root and clients prove changes off-chain) help minimize gas. When an update or revocation occurs, multiple changes can be bundled into one transaction to amortize cost. The design should also consider layer-2 solutions or permissioned sidechains for heavy transaction throughput, anchoring periodically to a main chain for security. Within smart contracts, efficient coding (Solidity optimizations, using bitmaps or minimal storage slots for status flags, etc.) can further cut gas fees. One study showed that using a single Merkle root update for many credentials is orders of magnitude cheaper than individual writes [cs.purdue.edu](https://cs.purdue.edu). By carefully designing how credential states are recorded (or by not recording at all on-chain beyond a hash), the DPP platform ensures cost won’t balloon as more passports are issued and modified.

## Cross-Chain Validation and Modular Contracts
The DPP smart contracts should be blockchain-agnostic, enabling deployment on both EVM chains (Ethereum, Polygon, EBSI’s Ethereum-based network, etc.) and non-EVM ledgers (e.g. Hyperledger Fabric, Tezos, etc.). A modular architecture is recommended: separate the credential logic (issuance, revocation, verification) from chain-specific implementations. For example, a core library of credential verification (in Solidity or WebAssembly) could be reused, while adapters handle the transaction finality and identity method of each chain. The goal is that a Verifiable Credential issued on one network can be trusted on another, through either cross-chain proofs or common DID resolution. One strategy is to use decentralized identity as an abstraction layer: e.g., a product’s DID Document could contain multiple service endpoints or blockchain proofs (one per chain), and verifiers simply check whichever chain they have access to. For cross-chain verification, the platform might leverage interoperability protocols or bridges – for instance, using a notary smart contract on Chain B that accepts a cryptographic proof (perhaps a Merkle proof or simplified payment verification) of an anchor that originally lives on Chain A. Another approach is to anchor the same Merkle root on multiple chains for redundancy, so a credential can be verified against any of them. The EBSI network, for example, is EVM-compatible (built on Hyperledger Besu) [hub.ebsi.eu](https://hub.ebsi.eu), meaning Solidity contracts on EBSI can interoperate with Ethereum tooling, but the platform might also need to support non-EVM national blockchains or consortium ledgers. By designing an extendable anchor registry interface, the DPP can plug in new blockchain connectors as needed (e.g. one module for EVM chains, one for a Substrate-based chain, etc.), ensuring the core credential format remains consistent. This modular, cross-chain approach not only avoids vendor lock-in but also increases resilience – if one ledger is down or too expensive, critical passport validations can occur on an alternative network. The smart contracts thus act as one component in a broader distributed trust architecture, rather than a monolithic on-chain system.

# Verifier and Credential Issuer Onboarding

## Becoming an EBSI-Compliant Issuer/Verifier
Onboarding as a trusted issuer or verifier in the European Blockchain Services Infrastructure (EBSI) ecosystem entails meeting legal, cryptographic, and procedural requirements. Legally, an organization must be recognized as a legitimate Trust Service Provider or accredited issuer under the EU’s framework. This starts with establishing a Decentralized Identifier (DID) for the legal entity (using the did:ebsi method or other allowed DID methods) and obtaining a Verifiable Authorisation to Onboard from the governing authority. In EBSI’s trust model, new issuers must receive a VerifiableAuthorisationToOnboard credential which allows them to anchor their DID Document on the EBSI DID Registry [hub.ebsi.eu](https://hub.ebsi.eu). This ensures the issuer’s DID is recorded on-chain and controlled by them (they prove ownership of the corresponding keys). Once onboarded, the entity may need an accreditation from a Root Trust Authority in the ecosystem. EBSI defines roles like Root Trusted Accreditation Organisations (Root TAOs), Trusted Accreditation Organisations (TAOs), and Trusted Issuers (TIs) [hub.ebsi.eu](https://hub.ebsi.eu) [hub.ebsi.eu](https://hub.ebsi.eu). In practice, this means an issuer (e.g., a certification body or a manufacturer) might first be accredited by a higher authority via a Verifiable Accreditation credential that specifies what types of credentials they can issue (for example, a ministry accredits a company to issue “Battery Compliance Certificates”). The issuer registers this in the Trusted Issuers Registry (TIR) on the blockchain [hub.ebsi.eu](https://hub.ebsi.eu) so that any verifier can cryptographically confirm the issuer’s legitimacy. Technically, the issuer must also prepare its infrastructure: they will need a secure wallet or HSM to manage signing keys, integration with the DPP platform’s APIs, and possibly run a node or use an API to write to the blockchain. For verifiers, onboarding focuses on trust consumption – they need to ensure they have the right root certificates and registry access. A verifier (e.g., customs officer’s app or a consumer’s wallet) will check that an issuer’s DID and credentials chain up to the European Trust Anchor. EBSI’s trust chain verification involves checking the issuer’s attestation from the European Trust Anchor (the EU Commission) and that the issuer appears in the Trusted Issuer Registry, as well as validating the credential’s signature and status [ogtechnologies.co](https://ogtechnologies.co) [ogtechnologies.co](https://ogtechnologies.co). Thus, any organization intending to verify DPP credentials may have to register for API access or software that can query EBSI’s registries (read access is public, but some verification software might require enrollment to get up-to-date trust lists). In summary, onboarding in an EU-aligned DPP network means proving who you are (through eIDAS-compliant identity), getting listed as trusted (through the blockchain-based registry and possibly legal agreements), and adhering to standards in how you issue or check credentials.

## TSP Integration and Qualified Signatures (eIDAS Compliance)
To align with EU regulations, the credentials in the DPP system should be signed with high-assurance keys. This typically involves using Trust Service Providers (TSPs) that issue Qualified Certificates for electronic seals or signatures under eIDAS (EU Regulation No. 910/2014). For an organization (like a manufacturer) issuing a product passport, this often means obtaining a Qualified Electronic Seal (QSeal) certificate – a digital certificate proving the organization’s identity, issued by an EU-authorized CA. The signing keys corresponding to this certificate are usually stored in a Hardware Security Module or provided via a Remote Signing Service by the TSP [ogtechnologies.co](https://ogtechnologies.co). The DPP platform can integrate with such a service so that every Verifiable Credential it issues is not only signed with a DID key, but also countersigned or backed by a qualified seal. This double-signing approach provides legal assurance: a Qualified Electronic Seal attached to a VC gives it the same legal status as a paper document stamped by the company [ogtechnologies.co](https://ogtechnologies.co). For issuers, the onboarding includes setting up this capability – e.g. using the InfoCert Remote QSeal Service or similar, which is explicitly designed to sign Verifiable Credentials with qualified certificates [impulse-h2020.eu](https://impulse-h2020.eu). Cryptographically, the platform might use the JSON Web Signature (JWS) or JSON-LD Signature format that can carry X.509 certificate data. The verifier software, in turn, will validate not just the VC’s integrity but also the certificate’s validity (checking it against EU Trusted Lists to ensure it’s a qualified certificate). This ensures that credentials are eIDAS compliant, meaning a verifier in any member state can trust the origin of the credential even if they don’t directly know the issuer – the signature itself is legally recognized. In practice, aligning with eIDAS may also require certain processes (the issuer might need to use a Qualified Signature Creation Device, and follow audit trails for when signatures are applied [entrust.com](https://entrust.com)). The DPP platform should enforce that only keys from a configured TSP are used for signing, and likely provide audit logs of credential issuance for compliance. By integrating trust services in this manner, the platform achieves a high level of assurance: credentials can stand up to regulatory scrutiny, carrying the presumption of authenticity and integrity across borders.

## Role of EUDI Wallets and OIDC4VC in Distribution
As Europe rolls out the European Digital Identity (EUDI) Wallet, the DPP platform is designed to interoperate with it for seamless credential distribution and inspection. In practical terms, this means using the OpenID Connect for Verifiable Credentials protocols (OIDC4VC) to issue and verify passports with user consent. For example, when a product is sold or a batch is manufactured, the issuer (or retailer) could offer the product’s passport to the buyer’s EUDI Wallet. The flow would work as follows: the wallet (acting as an OpenID client) receives a credential offer – perhaps through scanning a QR code on the product or an in-app notification. That QR code might encode an OpenID4VCI issuance initiation URL which the wallet uses to start the retrieval. The user would authenticate (possibly scanning a code or using their ID) and then the DPP platform’s issuer endpoint would send the Verifiable Credential into their wallet via a secure API [docs.igrant.io](https://docs.igrant.io) [docs.igrant.io](https://docs.igrant.io). All of this leverages standardized endpoints: an authorization server to get user consent, a Credential Issuance Endpoint to deliver the VC, etc., as defined by the OpenID4VCI spec. For verification, OIDC4VC’s counterpart (OpenID4VP) can be used. Suppose a recycling center or customs officer wants to verify a product’s passport. Using their app (verifier), they can present an OpenID4VP request (often via QR or NFC tap), which prompts the holder’s EUDI Wallet to generate a Verifiable Presentation of the passport credential. The presentation can be filtered (selective disclosure if needed, or combined with other credentials) and is sent back to the verifier’s system securely [docs.igrant.io](https://docs.igrant.io) [docs.igrant.io](https://docs.igrant.io). This flow ensures that the holder is in control and consents to sharing the data. In the background, the verifier’s system will validate the VP: checking the signatures (including that eIDAS seal), checking the issuer’s trust status (via EBSI TIR or similar), and the revocation status. The EUDI Wallet standards also emphasize privacy, so features like Selective Disclosure JWT (SD-JWT) or ZKP-based VPs may be in use [docs.igrant.io](https://docs.igrant.io) – e.g. a consumer can prove a product is compliant without revealing the entire passport. For the DPP platform, supporting EUDI wallets means implementing these endpoints and protocols, ensuring compliance with the Architecture Reference Framework (ARF) of the EU digital identity system [docs.igrant.io](https://docs.igrant.io). Additionally, the platform should handle OIDC credential offers – for instance, when a user scans a product’s QR, it might direct them to a web URL where they can authenticate and fetch the credential into their wallet. In summary, the DPP is not a standalone portal – it will plug into the broader wallet ecosystem, using OIDC4VC to push credentials to authorized wallets and OIDC4VP to let those credentials be verified by relying parties. This user-centric approach aligns with upcoming eIDAS 2.0 regulations and makes DPPs easily accessible: a battery passport or textile passport can live in the owner’s smartphone wallet, ready to be shown to a reseller, recycler, or inspector on demand.

# JSON-LD Schema Versioning & Credential Revocation

## Evolving JSON-LD Schemas with Backward Compatibility
As Digital Product Passport schemas evolve (new data fields, updated vocabularies, etc.), it’s critical to maintain backward compatibility so that older passports remain interpretable. A best practice is to version your JSON-LD contexts deliberately. For minor, additive changes, you can often update the existing context with new term definitions (JSON-LD allows adding terms without breaking older ones). For major changes, consider creating a new context URL (for example, dpp-schema-v2.jsonld) while still supporting the old one in verifiers. JSON-LD 1.1 introduced features like the @version tag and @protected to help manage context changes. Setting "@version": 1.1 in your context and marking terms as @protected prevents accidental override of important terms in extended contexts [w3c.github.io](https://w3c.github.io) [w3c.github.io](https://w3c.github.io). This means if a new context redefines a term that was marked protected, JSON-LD processors will throw an error – a useful safeguard to avoid silently altering the meaning of data. Use this to lock in critical terms (e.g., productID, manufacturer) so that any schema evolution doesn’t reinterpret them. For introducing new fields, design the schema such that unknown terms can be ignored by older software (the JSON-LD data model inherently allows ignoring undefined terms). For example, if version 1 of DPP schema has fields A, B, C and version 2 adds field D, an old client will just skip D. Never remove or repurpose fields in a non-backward-compatible way; instead deprecate gradually. If a field must be retired, you might keep it in the context but note it as deprecated (perhaps with an @deprecated annotation in documentation) and stop using it in new credentials, but maintain it for old ones. Another strategy is to namescape your terms by version, e.g., dpp:weight vs dpp_v2:weight if the definition changes meaning, though this can complicate consumers. Prefer to extend rather than redefine. Maintaining multiple context files is also an option: e.g., a base context for core terms and extension contexts per version or per industry. This modular approach allows combining contexts so that, for instance, a verifier might load both the base context and a context for “v2025 additions” to understand all terms. All JSON-LD contexts and schema definitions should be hosted at stable URLs, and older ones should remain accessible indefinitely. Cool URIs don’t change – once you publish a context at a URL, keep it live (or redirect appropriately) [w3c.github.io](https://w3c.github.io) [w3c.github.io](https://w3c.github.io). This ensures that even years later, a verifying party or a developer tool can retrieve the context and interpret an old credential. It’s wise to use a version identifier in the file name or path (e.g., /schemas/dpp/2024/context.jsonld for the 2024 version) so you can publish updates separately. In summary, plan a versioning policy from the start: support old schemas for their lifespan, publish clear changelogs for schema updates, and use JSON-LD features (@context, @version, @protected) to manage extensions safely.

## Credential Revocation and VC Status Architecture

### Status List 2021 vs. Cryptographic Accumulators
Revocation of verifiable credentials (VCs) in the DPP system can follow one of two primary models – a status list or an accumulator. The W3C Status List 2021 is a standard that uses a published bitstring or list to indicate which credentials are valid or revoked [docs.truvera.io](https://docs.truvera.io). In practice, an issuer maintains a JSON document (or on-chain data) with an array of bits, where each index corresponds to a credential (e.g. based on a nonce or index in the credential ID). Revoking a credential means flipping the corresponding bit to “1” (revoked). This approach is simple and widely interoperable [docs.truvera.io](https://docs.truvera.io) – anyone can fetch the status list (or query the on-chain registry) to check if a given ID is present. It also allows batch updates: an issuer can revoke many credentials by publishing one updated list, which is efficient when using a blockchain (one transaction updates multiple statuses). However, status lists have downsides: they reveal which credential was revoked (since the index or ID is exposed) and the list size grows linearly with the number of credentials [docs.truvera.io](https://docs.truvera.io). This is where cryptographic accumulators come in as an alternative. An accumulator (like a Bloom filter or pairing-based accumulator) allows an issuer to cryptographically “accumulate” all valid credential IDs into one short digest. Instead of checking a list, a verifier checks a proof (witness) that a given credential’s ID is a member of the accumulator set (for non-revocation) or was removed. Systems like Hyperledger Indy and others use accumulators to enable privacy-preserving revocation: the holder of a credential can be given a witness at issuance, and if the credential is later revoked, the witness will fail to validate. The Truvera documentation explains that accumulators can prove non-revocation without disclosing the credential ID, whereas a status list requires telling the verifier the exact ID to look up [docs.truvera.io](https://docs.truvera.io) [docs.truvera.io](https://docs.truvera.io). Accumulators also keep constant size (the accumulator value and witness are fixed size no matter how many credentials) [docs.truvera.io](https://docs.truvera.io). The trade-off is that managing witnesses and performing accumulator math can be complex. In the DPP context, if privacy is a major concern (e.g. a company doesn’t want a public list of all product IDs it revoked), an accumulator-based revocation registry could be used. If simplicity and interoperability are priority (e.g. regulators want a straightforward list to audit), Status List 2021 may suffice. The platform could even support both: for example, Status List 2021 for general use (since it’s W3C standard and understood by many wallets) [docs.truvera.io](https://docs.truvera.io), and an accumulator for cases requiring anonymity (the OG tech or Truvera approach is to use an accumulator for “anonymous credentials” where even revocation checks don’t reveal which credential is being checked [docs.truvera.io](https://docs.truvera.io) [docs.truvera.io](https://docs.truvera.io)). It’s worth noting that using a status list on-chain can be efficient when batching, but if many single revocations are done, a specialized accumulator contract might incur less gas overall by not growing in storage per entry. The design decision may be to implement a VC Status Registry interface abstractly, and allow plugging in different backends (bit list or accumulator), based on credential type or issuer preference.

### Federated vs. Self-Sovereign Revocation Strategies
In a federated revocation model, a central authority or network maintains the revocation list for many issuers. An example is the EBSI Trusted Issuer Registry and Status Registry – a public ledger where any verifier can look up if a credential (by ID or hash) has been revoked by its issuer [ogtechnologies.co](https://ogtechnologies.co) [ogtechnologies.co](https://ogtechnologies.co). Federated models often come with governance: e.g., the network might require issuers to publish revocations to a common contract, ensuring consistency. This aids auditability – regulators can see all revoked credentials in one place. On the other hand, a self-sovereign revocation approach means each issuer is responsible for hosting or publishing revocation data for their own credentials. For instance, an issuer could include a URL in each credential pointing to a JSON status file they host; or they run their own smart contract keyed to their DID for revocations. This maximizes issuer control and can reduce dependency on a central service, but verifiers must know how to find and trust each issuer’s revocation mechanism (which can be addressed by standardizing the credentialStatus field in the VC to point to either a global or issuer-specific registry). The DPP platform leans towards a hybrid: use a shared ledger or registry for critical credentials (ensuring any stakeholder can quickly check status in one query) but also allow issuers to plug in their own revocation endpoints if they operate outside the common network. Self-sovereign methods align with the SSI principle that issuers control their data, but practically, for product passports that will be checked by many parties over a long supply chain, having a unified “phone book” of revocations is very helpful. One strategy is a federated network of status lists: e.g., the platform could maintain a distributed database where each issuer has a namespace, but queries can be aggregated through a single API. This is similar to how domain names work – decentralized responsibility but centralized resolution. In summary, federated revocation = easier lookup and governance (but possibly more overhead to maintain globally), self-sovereign revocation = more freedom (but requires robust discovery mechanisms). The DPP architecture document can outline how issuers integrate either approach, and verifiers always go through the standard interface (resolving the status given the info in the credential). By planning for both, the platform remains flexible as the ecosystem grows.

### Credential Lifecycle: Archival vs. Mutable Credentials
Product passports will often have a long lifespan, accumulating information from production to end-of-life. This raises the question: do we treat the VC as immutable snapshots that get superseded, or do we update one credential continuously? The W3C VC Data Model generally assumes credentials are tamper-evident and static once issued (you can always issue a new version, but you cannot change the signed data without invalidating the signature [github.com](https://github.com)). Therefore, one straightforward approach is delta issuance: whenever something about the product changes (ownership, repair, added carbon footprint data), the platform issues a new VC (or a new Verifiable Presentation containing the updated claims) and perhaps revokes the previous one. This provides an audit trail, as old versions can be archived. However, constantly re-issuing credentials could lead to many credentials per item, complicating tracking. An alternative concept is a composite or dynamic credential. For example, the DPP could be implemented as a collection of sub-credentials (each covering a data category like materials, repairs, etc.), all linked to the product’s DID. Updates then mean adding or revoking sub-credentials, rather than replacing the whole passport. Another approach is using linked data patches – essentially having one credential that contains a reference to a verifiable log of changes (the credential’s subject could be a product DID and the credential could include a hash of the current product data, which gets updated via a chain of linked transactions). The ZVEI DPP4.0 concept leans on an Asset Administration Shell (AAS) model where the DPP data is modular and can be signed in parts [zvei.org](https://zvei.org). That implies each “submodel” (like a module for sustainability data) could be signed by whoever is responsible (and updated independently). For long-term archival, the system should preserve historical records: every update event could be stored (on-chain as events or off-chain in an append-only log) with timestamps. This gives a chronological audit trail of the passport’s evolution – useful for compliance and investor due diligence (proving that certain data was present at a given time and not manipulated later). It might be wise to incorporate a versioning mechanism in credential IDs or metadata (e.g. a version number or effective date field). When a credential is superseded, the new one could carry a reference to the prior version (and the system could automatically mark the old as revoked or expired). This is analogous to a passport document in real life that gets renewed: the old one is invalid but kept on file. Lifecycle control also means deciding when a credential should expire. For instance, a digital passport might be set to expire after 10 years, or when the product is definitively recycled, whichever comes first – at which point no further updates are expected. Expiry combined with revocation ensures that credentials don’t live forever unused. In terms of mutable pathways, some industry proposals suggest the manufacturer could host an online data record that is always up-to-date, and the VC just authenticates the link to it. But that reintroduces centralization and reliance on the manufacturer’s database. A compromise is using the blockchain as a verifiable log of changes: e.g., each significant update (like “battery capacity after 3 years”) could be anchored as a small event, and the current credential could just reference the latest state hash (which a verifier can cross-check against the log). The platform documentation should clearly outline the expected lifecycle: e.g., “Upon product manufacture, a Version 1 DPP credential is issued. When the product is serviced or its status changes, a new credential (Version 2) is issued and the old one is set to expired in the status registry. All versions remain accessible for audit, but only the latest is considered active.” This ensures long-term archival (nothing is lost; investors or auditors can see the history) while making it easy for operational verifiers to get the current truth. The design must also consider storage: if each product ends up with 10 credentials over life, the system and wallets need to manage that gracefully (perhaps bundling them or showing only current unless deep audit is needed). By planning for delta updates and controlled mutability, the DPP platform can accommodate the dynamic nature of product data without sacrificing the verifiability and integrity guarantees of the VC model.

# Multilingual and Internationalized DPP Documents
Digital Product Passports will be used across borders, so the schema and data should accommodate multiple languages and locales. JSON-LD has a built-in support for language-tagged strings and even direction (ltr/rtl) markers. To internationalize text fields, define them in the context with a language mapping container. For example, a property like "productDescription" can be defined to allow a language map:
```json
"productDescription": { 
  "@id": "ex:description", 
  "@container": "@language" 
}
```
This means in the credential, productDescription can be an object with language keys: e.g., "productDescription": {"en": "High-efficiency solar panel", "de": "Hocheffizientes Solarmodul"}. Consumers will then pick the appropriate language. By indicating a default language in the context (using @language), you can also supply a default for strings without an explicit tag [stackoverflow.com](https://stackoverflow.com). There are no restrictions on language in JSON-LD data – the keys (property names) are IRIs or terms, and values can be in any language as long as appropriately tagged [stackoverflow.com](https://stackoverflow.com). We recommend providing translations for key human-readable fields (like product category names, compliance descriptions, etc.) within the DPP or via linked resources. For instance, if the DPP includes a code that represents a material type, that code could be resolvable to a concept that has rdfs:label in multiple languages in an ontology. Leverage linked data vocabularies that are already internationalized. Schema.org, for example, provides translations of terms in many languages. If suitable, mapping DPP fields to schema.org or other standard vocabularies can instantly grant multi-language support for those terms. Similarly, use controlled code lists (for product categories, materials, etc.) that publish labels in multiple languages. The DPP platform could incorporate an i18n module where all UI-facing strings or common taxonomy terms are stored with translations, ensuring that when a passport is displayed, it can show content in the user’s language. Another consideration is units and regional formats (though not strictly a language issue). Ensure that numeric data like dimensions or weight are accompanied by units (possibly SI units per regulation) so they’re unambiguous internationally. If necessary, the schema can allow multiple values with unit qualifiers (e.g., weight in kg and in local unit). However, typically the passport will standardize units to avoid confusion. For directionality, JSON-LD 1.1 supports @direction, though most product data text won’t require this unless including right-to-left scripts. In summary, to achieve multilingual DPPs: design the schema to accept language maps for text, use context @language to set defaults, pick vocabularies and code lists that come with translations, and possibly provide your own translation files for any static vocabulary. This ensures the same DPP can be understood by a German regulator or a French consumer just as well as by an English speaker, improving usability and compliance across markets. The underlying linked-data graph stays the same; it’s just enriched with language-tagged literals.

# Credential Revocation Strategies
Managing the status and revocation of Verifiable Credentials is essential, especially for compliance documents like DPPs which might be updated or invalidated over time. The platform should implement a Credential Status mechanism as per W3C Verifiable Credentials recommendations. A common approach is the Status List 2021 (also known as bitstring status list) method: each credential carries a credentialStatus field pointing to a URL (or DID URL) that contains the status information. For high-volume scenarios, a single status list (a bit array where each bit corresponds to a credential) is efficient. EBSI, for example, endorses this approach – issuers maintain a credential status list and assign each credential a bit position in that list [hub.ebsi.eu](https://hub.ebsi.eu). The credentialStatus object in the DPP VC would include an identifier for the status list and an index (bit position). A 0 bit means the credential is valid, a 1 means revoked (or suspended) [hub.ebsi.eu](https://hub.ebsi.eu). When an issuer needs to revoke a DPP (say the product was recalled or data was found erroneous), they update their status list by flipping the corresponding bit. Verifiers can fetch this list (or a segment of it) and check the bit. Because the list can be hosted behind a privacy-preserving status proxy (as EBSI does) [hub.ebsi.eu](https://hub.ebsi.eu), verifiers don’t necessarily learn which credential is being checked – they just query by index. The DPP platform should provide tooling to manage these status lists or registries. This could mean hosting a simple API endpoint that returns a status list (which might be a compressed bitset or a JSON with indices of revoked credentials). Issuers might also choose to use an on-chain revocation registry (for instance, writing a transaction to a smart contract to mark a credential ID revoked). On-chain registries offer transparency and immutability, but have cost and privacy implications. Status lists (off-chain) are more scalable – a single bitlist can indicate status for thousands of credentials in one small file, and it can be signed by the issuer for authenticity. Another method is Revocation by reference: the credential could include an ID that gets published to a revocation registry if revoked. For example, some DID methods use a revocation registry entry or a Blockchain transaction to signal revocation. The trade-off is that checking revocation then requires consulting that blockchain or registry. For the EU’s purposes, the status list (with a proxy) approach is preferred to minimize correlation (the verifier doesn’t necessarily know which credential is being checked, since the proxy can handle requests in a way that hides the exact index being queried) [hub.ebsi.eu](https://hub.ebsi.eu). Additionally, expiration is a simple but important mechanism: every Verifiable Credential can have an expirationDate. The DPP credentials should have a reasonable validity period set (depending on the product type and regulations). For instance, a battery passport might be valid for the life of the battery or until a certain compliance deadline. Once past expiration, a credential is considered invalid by default. Expiration is not revocation per se, but it ensures credentials aren’t used beyond their intended timeframe. It’s straightforward for verifiers to check a timestamp. The platform should support revocation updates and notifications. If a DPP is revoked, ideally affected parties (e.g., the manufacturer, maybe the product owner) can be notified. One way is to leverage the DID Comm or webhook subscriptions to let certain participants know when a status changes. Internally, maintain an audit log of revocations: who revoked, when, and why – this is crucial for compliance traceability. In summary, implement a robust VC status system: likely by using W3C Status List 2021 spec (bit arrays) [hub.ebsi.eu](https://hub.ebsi.eu), hosted via a secure API; ensure every credential includes a status pointer; and support credential expiration and possibly on-chain registry integration for an extra layer of trust. This ensures verifiers can reliably detect invalidated passports (preventing fraud or misuse of revoked credentials) and that issuers have the tools to manage the lifecycle of credentials after issuance.

# Global Standards Harmonization

### Schema conflicts
Different bodies (UNECE/UNTP, GS1, ISO, OECD, etc.) each define DPP-related schemas and codes that sometimes overlap or diverge. To resolve conflicts, start by aligning on core concepts (e.g. product identifiers, substances, material composition). Wherever standards differ, map equivalent fields or adopt a superset schema. For example, the UNTP Digital Product Passport specification explicitly notes it “does not conflict” with the EU (national) DPP regulations [uncefact.github.io](https://uncefact.github.io), essentially using a minimal data model upstream of country-specific rules. In practice, harmonization means reusing common codes: use GS1 GTIN/GLN for products and parties, ISO codes for units/countries, UN values for waste categories, etc. Document any translations between systems in a version-controlled mapping.

### Namespace alignment
Use JSON-LD to bridge vocabularies. Define a JSON-LD @context that maps each term to a fully-qualified URI in a chosen ontology (e.g. Schema.org, GS1 Web Vocabulary, UNTP DPP model, ISO/IEC 19987 traceability vocabulary, W3C DCAT, etc.). By anchoring all data elements to IRIs, clients can understand semantics even if different standards use different names. For example, GS1 guidance emphasizes “common semantics” so that different terms must be the same or mappable [gs1.eu](https://gs1.eu). In JSON-LD, one could map a GS1 gtin and an ISO productId both to schema:Product/sku. GS1 explains that JSON-LD contexts make it possible to mix terms from different vocabularies in one document – e.g. data can use schema.org, SEMIC, GS1, UNTP terms together [gs1.eu](https://gs1.eu). This lets producers express a DPP using whichever codes their systems have, while still remaining interoperable.

### Semantic interoperability
Follow the EU’s recommendation to use a shared “semantic anchor” such as Schema.org [gs1.eu](https://gs1.eu). In practice, pick a small set of well-known RDF vocabularies and ensure each data field in your JSON-LD has a clear mapping. Maintain composite contexts: for instance, import the UN Sustainability Vocabulary for ESG claims, GS1 Web Vocabulary for product data, and UNTP DPP ontology for passport structure. Use SHACL (or JSON Schema) to validate that combined data conform to all expected vocabularies. As GS1 notes, JSON-LD naturally forms a mini knowledge-graph, so mix-and-match works: one company’s colorHue can map to schema:color, another’s shade can do the same, via contexts [gs1.eu](https://gs1.eu). This semantic layering (often called linked data) is the recommended approach: GS1 documents show JSON-LD contexts linking multiple vocabularies, including UNTP’s model, to ensure DPP data merges seamlessly [gs1.eu](https://gs1.eu).

### Practical strategy
Establish a governance process for vocabularies and namespaces. Whenever UNECE, GS1 or others update their models, review and update the JSON-LD contexts accordingly. Host context files at stable versioned URIs (e.g. including a version in the path), so old and new terms can be distinguished. Wherever possible, reuse existing code lists (e.g. GS1’s GS1 PPC or SEMIC core vocabularies). If conflicts arise (e.g. two standards define “material” differently), convene a mapping committee (or use schema.org as a fallback anchor [gs1.eu](https://gs1.eu)). In sum, leverage JSON-LD context composition and linked-data vocabularies to achieve interoperability: JSON-LD allows each payload to self-describe its terms, and contexts can be composed or extended without breaking existing clients [gs1.eu](https://gs1.eu).
