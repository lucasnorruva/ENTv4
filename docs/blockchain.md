# Smart Contract Deployment and Blockchain Anchoring

We anchor product integrity on the blockchain by storing cryptographic hashes of the product data. This provides a tamper-proof, time-stamped record of the product's state at any given point.

## Core Anchoring Strategy

The simplest method is to store a product's data hash on an EVM-compatible chain like Polygon PoS. The low transaction costs and fast finality make it ideal. A backend service, like a Firebase Cloud Function, is triggered when a passport is finalized. It serializes the product metadata (e.g., as JSON-LD), computes its `keccak256` hash, and calls a smart contract function to record it.

### Smart Contract Example: ProductRegistry

Below is a sample Solidity contract that records a product’s hash and emits an event.

```solidity
// SPDX-License-Identifier: MIT
pragma solidity ^0.8.0;

contract ProductRegistry {
    // Mapping from product ID to the stored hash (e.g. keccak256 of product metadata)
    mapping(uint256 => bytes32) public productHash;

    // Event emitted when a product hash is anchored on-chain
    event ProductAnchored(uint256 indexed productId, bytes32 hashValue);

    // Anchor the hash of arbitrary data for a given productId
    function anchorProductHash(uint256 productId, bytes calldata data) external {
        bytes32 h = keccak256(data);
        productHash[productId] = h;
        emit ProductAnchored(productId, h);
    }
}
```

This approach provides a solid foundation for data integrity verification.

## Advanced Concepts & EU Alignment

For deeper integration and alignment with EU frameworks like EBSI, we employ more advanced strategies including Verifiable Credentials (VCs), Decentralized Identifiers (DIDs), and NFTs.

### Cross-chain Validation and Modular Contracts
The DPP smart contracts should be blockchain-agnostic, enabling deployment on both EVM chains (Ethereum, Polygon, EBSI’s Ethereum-based network, etc.) and non-EVM ledgers (e.g. Hyperledger Fabric, Tezos, etc.). A modular architecture is recommended: separate the credential logic (issuance, revocation, verification) from chain-specific implementations. For example, a core library of credential verification (in Solidity or WebAssembly) could be reused, while adapters handle the transaction finality and identity method of each chain. The goal is that a Verifiable Credential issued on one network can be trusted on another, through either cross-chain proofs or common DID resolution. One strategy is to use decentralized identity as an abstraction layer: e.g., a product’s DID Document could contain multiple service endpoints or blockchain proofs (one per chain), and verifiers simply check whichever chain they have access to. For cross-chain verification, the platform might leverage interoperability protocols or bridges – for instance, using a notary smart contract on Chain B that accepts a cryptographic proof (perhaps a Merkle proof or simplified payment verification) of an anchor that originally lives on Chain A. Another approach is to anchor the same Merkle root on multiple chains for redundancy, so a credential can be verified against any of them. The EBSI network, for example, is EVM-compatible (built on Hyperledger Besu) [hub.ebsi.eu](https://hub.ebsi.eu), meaning Solidity contracts on EBSI can interoperate with Ethereum tooling, but the platform might also need to support non-EVM national blockchains or consortium ledgers. By designing an extendable anchor registry interface, the DPP can plug in new blockchain connectors as needed (e.g. one module for EVM chains, one for a Substrate-based chain, etc.), ensuring the core credential format remains consistent. This modular, cross-chain approach not only avoids vendor lock-in but also increases resilience – if one ledger is down or too expensive, critical passport validations can occur on an alternative network. The smart contracts thus act as one component in a broader distributed trust architecture, rather than a monolithic on-chain system.

### Solidity contract examples for VCs
A more advanced pattern uses issuer/verifier contracts for W3C Verifiable Credentials.

**Issuer Contract:**
```solidity
contract VCIssuer {
    mapping(address => bool) public issuers;
    mapping(address => mapping(string => string)) public creds;  // holder → (key → value)
    event Issued(address indexed issuer, address indexed holder, string key, string value);

    constructor() { issuers[msg.sender] = true; }
    modifier onlyIssuer() { require(issuers[msg.sender]); _; }

    function addIssuer(address newIssuer) external onlyIssuer {
        issuers[newIssuer] = true;
    }
    function issueCred(address holder, string calldata key, string calldata value) external onlyIssuer {
        creds[holder][key] = value;
        emit Issued(msg.sender, holder, key, value);
    }
}
```

**Verifier Contract:**
```solidity
contract VCVerifier {
    mapping(address => bool) public verifiers;
    event Verified(address indexed verifier, address indexed subject, string key, string value);

    constructor() { verifiers[msg.sender] = true; }
    modifier onlyVerifier() { require(verifiers[msg.sender]); _; }

    function addVerifier(address v) external onlyVerifier {
        verifiers[v] = true;
    }
    function verifyCred(address subject, string calldata key, string calldata expected) external onlyVerifier {
        // Note: This requires a way to reference the VCIssuerContract instance
        // string memory actual = VCIssuerContract(issuerAddress).creds(subject, key);
        // require(keccak256(bytes(actual)) == keccak256(bytes(expected)), "Verification failed");
        // emit Verified(msg.sender, subject, key, actual);
    }
}
```

### NFT/DID architecture
Each physical product can be represented by a unique NFT (ERC-721/1155) serving as its “digital twin.” The NFT metadata links to the DPP data, which is structured as a Verifiable Credential. The product’s globally unique identifier is a Decentralized Identifier (e.g., `did:web`, `did:ebsi`), linking the on-chain asset (NFT) to its off-chain data (VC).

### Verifiable Credential workflows
All DPPs are implemented as W3C Verifiable Credentials. The manufacturer (issuer) creates a JSON-LD credential, signs it with their private key, and makes it resolvable via the product's DID. This aligns with EU Digital Identity Wallet standards and ensures interoperability.

# Compliance Document Structuring

## JSON-LD and Semantic Web compliance
To maximize interoperability, all product passports are structured in JSON-LD (JSON for Linked Data). This allows every data point (e.g., a chemical ID) to be tied to a global, semantic definition (like a GS1 or UNECE vocabulary), making the data machine-readable and verifiable.

## Regulatory alignment (REACH, RoHS, ESPR, etc.)
The DPP schema is designed to directly map to the data requirements of major EU regulations. Fields for RoHS hazardous substances, REACH SVHCs, and ESPR-mandated data are all first-class citizens in the JSON-LD structure. This allows for automated compliance checking against regulatory thresholds.

## Credential signing workflow
Each DPP document is cryptographically signed to prevent tampering. We use JSON-LD Proofs, which are compatible with the EU's trust framework (e.g., eIDAS). In our system, an “EU-verifier” node, conforming to EBSI rules, can sign the final composite DPP as a Verifiable Credential and anchor its hash on the EBSI ledger. This provides a high degree of trust and legal non-repudiation.

# Smart Contract Architecture and Anchoring Models

## Anchor Aggregation with Merkle Trees and ZK Proofs
The DPP platform’s smart contracts should minimize on-chain operations by aggregating credential anchors. One approach is to use Merkle tree batching: hash each credential (or update) into a Merkle tree and only publish the root on-chain [cs.purdue.edu](https://cs.purdue.edu). This method allows thousands of credentials to share one anchor, drastically reducing gas costs (e.g. anchoring each VC individually might cost ~576k gas, versus ~78k gas for a single Merkle root that covers many credentials [cs.purdue.edu](https://cs.purdue.edu)). To preserve trust, off-chain proofs can be generated – a holder provides a Merkle inclusion proof that their credential was part of the anchored root, and verifiers can validate it against the on-chain root. Additionally, zero-knowledge proofs (ZKPs) can attest compliance without exposing data. Using ZK-SNARKs or similar, an issuer can prove a product meets certain standards (e.g. contains < X% of a restricted substance) without revealing the underlying proprietary data [circularise.com](https://circularise.com). This “proof-of-compliance” via ZKP balances transparency and IP protection, enabling verifiers to trust regulatory claims while sensitive product data remains confidential [circularise.com](https://circularise.com). The smart contracts would simply verify the ZK proof on-chain (or an off-chain service would do so and anchor a certificate), avoiding large data payloads and ensuring privacy.

## Gas Optimization in Credential Workflows
Every on-chain interaction (issuance, update, revocation) must be optimized for cost, especially if scaled to millions of products. By leveraging batched operations and compact data structures, the platform avoids redundant writes. For instance, instead of writing a new state for each credential update, the contract could store only a reference to a rolling Merkle root or use event logs for off-chain aggregation. Techniques like stateless proofs (where the chain stores a constant root and clients prove changes off-chain) help minimize gas. When an update or revocation occurs, multiple changes can be bundled into one transaction to amortize cost. The design should also consider layer-2 solutions or permissioned sidechains for heavy transaction throughput, anchoring periodically to a main chain for security. Within smart contracts, efficient coding (Solidity optimizations, using bitmaps or minimal storage slots for status flags, etc.) can further cut gas fees. One study showed that using a single Merkle root update for many credentials is orders of magnitude cheaper than individual writes [cs.purdue.edu](https://cs.purdue.edu). By carefully designing how credential states are recorded (or by not recording at all on-chain beyond a hash), the DPP platform ensures cost won’t balloon as more passports are issued and modified.

## Cross-Chain Validation and Modular Contracts
The DPP smart contracts should be blockchain-agnostic, enabling deployment on both EVM chains (Ethereum, Polygon, EBSI’s Ethereum-based network, etc.) and non-EVM ledgers (e.g. Hyperledger Fabric, Tezos, etc.). A modular architecture is recommended: separate the credential logic (issuance, revocation, verification) from chain-specific implementations. For example, a core library of credential verification (in Solidity or WebAssembly) could be reused, while adapters handle the transaction finality and identity method of each chain. The goal is that a Verifiable Credential issued on one network can be trusted on another, through either cross-chain proofs or common DID resolution. One strategy is to use decentralized identity as an abstraction layer: e.g., a product’s DID Document could contain multiple service endpoints or blockchain proofs (one per chain), and verifiers simply check whichever chain they have access to. For cross-chain verification, the platform might leverage interoperability protocols or bridges – for instance, using a notary smart contract on Chain B that accepts a cryptographic proof (perhaps a Merkle proof or simplified payment verification) of an anchor that originally lives on Chain A. Another approach is to anchor the same Merkle root on multiple chains for redundancy, so a credential can be verified against any of them. The EBSI network, for example, is EVM-compatible (built on Hyperledger Besu) [hub.ebsi.eu](https://hub.ebsi.eu), meaning Solidity contracts on EBSI can interoperate with Ethereum tooling, but the platform might also need to support non-EVM national blockchains or consortium ledgers. By designing an extendable anchor registry interface, the DPP can plug in new blockchain connectors as needed (e.g. one module for EVM chains, one for a Substrate-based chain, etc.), ensuring the core credential format remains consistent. This modular, cross-chain approach not only avoids vendor lock-in but also increases resilience – if one ledger is down or too expensive, critical passport validations can occur on an alternative network. The smart contracts thus act as one component in a broader distributed trust architecture, rather than a monolithic on-chain system.

# Verifier and Credential Issuer Onboarding

## Becoming an EBSI-Compliant Issuer/Verifier
Onboarding as a trusted issuer or verifier in the European Blockchain Services Infrastructure (EBSI) ecosystem entails meeting legal, cryptographic, and procedural requirements. Legally, an organization must be recognized as a legitimate Trust Service Provider or accredited issuer under the EU’s framework. This starts with establishing a Decentralized Identifier (DID) for the legal entity (using the did:ebsi method or other allowed DID methods) and obtaining a Verifiable Authorisation to Onboard from the governing authority. In EBSI’s trust model, new issuers must receive a VerifiableAuthorisationToOnboard credential which allows them to anchor their DID Document on the EBSI DID Registry [hub.ebsi.eu](https://hub.ebsi.eu). This ensures the issuer’s DID is recorded on-chain and controlled by them (they prove ownership of the corresponding keys). Once onboarded, the entity may need an accreditation from a Root Trust Authority in the ecosystem. EBSI defines roles like Root Trusted Accreditation Organisations (Root TAOs), Trusted Accreditation Organisations (TAOs), and Trusted Issuers (TIs) [hub.ebsi.eu](https://hub.ebsi.eu) [hub.ebsi.eu](https://hub.ebsi.eu). In practice, this means an issuer (e.g., a certification body or a manufacturer) might first be accredited by a higher authority via a Verifiable Accreditation credential that specifies what types of credentials they can issue (for example, a ministry accredits a company to issue “Battery Compliance Certificates”). The issuer registers this in the Trusted Issuers Registry (TIR) on the blockchain [hub.ebsi.eu](https://hub.ebsi.eu) so that any verifier can cryptographically confirm the issuer’s legitimacy. Technically, the issuer must also prepare its infrastructure: they will need a secure wallet or HSM to manage signing keys, integration with the DPP platform’s APIs, and possibly run a node or use an API to write to the blockchain. For verifiers, onboarding focuses on trust consumption – they need to ensure they have the right root certificates and registry access. A verifier (e.g., customs officer’s app or a consumer’s wallet) will check that an issuer’s DID and credentials chain up to the European Trust Anchor. EBSI’s trust chain verification involves checking the issuer’s attestation from the European Trust Anchor (the EU Commission) and that the issuer appears in the Trusted Issuer Registry, as well as validating the credential’s signature and status [ogtechnologies.co](https://ogtechnologies.co) [ogtechnologies.co](https://ogtechnologies.co). Thus, any organization intending to verify DPP credentials may have to register for API access or software that can query EBSI’s registries (read access is public, but some verification software might require enrollment to get up-to-date trust lists). In summary, onboarding in an EU-aligned DPP network means proving who you are (through eIDAS-compliant identity), getting listed as trusted (through the blockchain-based registry and possibly legal agreements), and adhering to standards in how you issue or check credentials.

## TSP Integration and Qualified Signatures (eIDAS Compliance)
To align with EU regulations, the credentials in the DPP system should be signed with high-assurance keys. This typically involves using Trust Service Providers (TSPs) that issue Qualified Certificates for electronic seals or signatures under eIDAS (EU Regulation No. 910/2014). For an organization (like a manufacturer) issuing a product passport, this often means obtaining a Qualified Electronic Seal (QSeal) certificate – a digital certificate proving the organization’s identity, issued by an EU-authorized CA. The signing keys corresponding to this certificate are usually stored in a Hardware Security Module or provided via a Remote Signing Service by the TSP [ogtechnologies.co](https://ogtechnologies.co). The DPP platform can integrate with such a service so that every Verifiable Credential it issues is not only signed with a DID key, but also countersigned or backed by a qualified seal. This double-signing approach provides legal assurance: a Qualified Electronic Seal attached to a VC gives it the same legal status as a paper document stamped by the company [ogtechnologies.co](https://ogtechnologies.co). For issuers, the onboarding includes setting up this capability – e.g. using the InfoCert Remote QSeal Service or similar, which is explicitly designed to sign Verifiable Credentials with qualified certificates [impulse-h2020.eu](https://impulse-h2020.eu). Cryptographically, the platform might use the JSON Web Signature (JWS) or JSON-LD Signature format that can carry X.509 certificate data. The verifier software, in turn, will validate not just the VC’s integrity but also the certificate’s validity (checking it against EU Trusted Lists to ensure it’s a qualified certificate). This ensures that credentials are eIDAS compliant, meaning a verifier in any member state can trust the origin of the credential even if they don’t directly know the issuer – the signature itself is legally recognized. In practice, aligning with eIDAS may also require certain processes (the issuer might need to use a Qualified Signature Creation Device, and follow audit trails for when signatures are applied [entrust.com](https://entrust.com)). The DPP platform should enforce that only keys from a configured TSP are used for signing, and likely provide audit logs of credential issuance for compliance. By integrating trust services in this manner, the platform achieves a high level of assurance: credentials can stand up to regulatory scrutiny, carrying the presumption of authenticity and integrity across borders.

## Role of EUDI Wallets and OIDC4VC in Distribution
As Europe rolls out the European Digital Identity (EUDI) Wallet, the DPP platform is designed to interoperate with it for seamless credential distribution and inspection. In practical terms, this means using the OpenID Connect for Verifiable Credentials protocols (OIDC4VC) to issue and verify passports with user consent. For example, when a product is sold or a batch is manufactured, the issuer (or retailer) could offer the product’s passport to the buyer’s EUDI Wallet. The flow would work as follows: the wallet (acting as an OpenID client) receives a credential offer – perhaps through scanning a QR code on the product or an in-app notification. That QR code might encode an OpenID4VCI issuance initiation URL which the wallet uses to start the retrieval. The user would authenticate (possibly scanning a code or using their ID) and then the DPP platform’s issuer endpoint would send the Verifiable Credential into their wallet via a secure API [docs.igrant.io](https://docs.igrant.io) [docs.igrant.io](https://docs.igrant.io). All of this leverages standardized endpoints: an authorization server to get user consent, a Credential Issuance Endpoint to deliver the VC, etc., as defined by the OpenID4VCI spec. For verification, OIDC4VC’s counterpart (OpenID4VP) can be used. Suppose a recycling center or customs officer wants to verify a product’s passport. Using their app (verifier), they can present an OpenID4VP request (often via QR or NFC tap), which prompts the holder’s EUDI Wallet to generate a Verifiable Presentation of the passport credential. The presentation can be filtered (selective disclosure if needed, or combined with other credentials) and is sent back to the verifier’s system securely [docs.igrant.io](https://docs.igrant.io) [docs.igrant.io](https://docs.igrant.io). This flow ensures that the holder is in control and consents to sharing the data. In the background, the verifier’s system will validate the VP: checking the signatures (including that eIDAS seal), checking the issuer’s trust status (via EBSI TIR or similar), and the revocation status. The EUDI Wallet standards also emphasize privacy, so features like Selective Disclosure JWT (SD-JWT) or ZKP-based VPs may be in use [docs.igrant.io](https://docs.igrant.io) – e.g. a consumer can prove a product is compliant without revealing the entire passport. For the DPP platform, supporting EUDI wallets means implementing these endpoints and protocols, ensuring compliance with the Architecture Reference Framework (ARF) of the EU digital identity system [docs.igrant.io](https://docs.igrant.io). Additionally, the platform should handle OIDC credential offers – for instance, when a user scans a product’s QR, it might direct them to a web URL where they can authenticate and fetch the credential into their wallet. In summary, the DPP is not a standalone portal – it will plug into the broader wallet ecosystem, using OIDC4VC to push credentials to authorized wallets and OIDC4VP to let those credentials be verified by relying parties. This user-centric approach aligns with upcoming eIDAS 2.0 regulations and makes DPPs easily accessible: a battery passport or textile passport can live in the owner’s smartphone wallet, ready to be shown to a reseller, recycler, or inspector on demand.

# JSON-LD Schema Versioning & Credential Revocation

## Evolving JSON-LD Schemas with Backward Compatibility
As Digital Product Passport schemas evolve (new data fields, updated vocabularies, etc.), it’s critical to maintain backward compatibility so that older passports remain interpretable. A best practice is to version your JSON-LD contexts deliberately. For minor, additive changes, you can often update the existing context with new term definitions (JSON-LD allows adding terms without breaking older ones). For major changes, consider creating a new context URL (for example, dpp-schema-v2.jsonld) while still supporting the old one in verifiers. JSON-LD 1.1 introduced features like the @version tag and @protected to help manage context changes. Setting "@version": 1.1 in your context and marking terms as @protected prevents accidental override of important terms in extended contexts [w3c.github.io](https://w3c.github.io) [w3c.github.io](https://w3c.github.io). This means if a new context redefines a term that was marked protected, JSON-LD processors will throw an error – a useful safeguard to avoid silently altering the meaning of data. Use this to lock in critical terms (e.g., productID, manufacturer) so that any schema evolution doesn’t reinterpret them. For introducing new fields, design the schema such that unknown terms can be ignored by older software (the JSON-LD data model inherently allows ignoring undefined terms). For example, if version 1 of DPP schema has fields A, B, C and version 2 adds field D, an old client will just skip D. Never remove or repurpose fields in a non-backward-compatible way; instead deprecate gradually. If a field must be retired, you might keep it in the context but note it as deprecated (perhaps with an @deprecated annotation in documentation) and stop using it in new credentials, but maintain it for old ones. Another strategy is to namescape your terms by version, e.g., dpp:weight vs dpp_v2:weight if the definition changes meaning, though this can complicate consumers. Prefer to extend rather than redefine. Maintaining multiple context files is also an option: e.g., a base context for core terms and extension contexts per version or per industry. This modular approach allows combining contexts so that, for instance, a verifier might load both the base context and a context for “v2025 additions” to understand all terms. All JSON-LD contexts and schema definitions should be hosted at stable URLs, and older ones should remain accessible indefinitely. Cool URIs don’t change – once you publish a context at a URL, keep it live (or redirect appropriately) [w3c.github.io](https://w3c.github.io) [w3c.github.io](https://w3c.github.io). This ensures that even years later, a verifying party or a developer tool can retrieve the context and interpret an old credential. It’s wise to use a version identifier in the file name or path (e.g., /schemas/dpp/2024/context.jsonld for the 2024 version) so you can publish updates separately. In summary, plan a versioning policy from the start: support old schemas for their lifespan, publish clear changelogs for schema updates, and use JSON-LD features (@context, @version, @protected) to manage extensions safely.

## Credential Revocation and VC Status Architecture

### Status List 2021 vs. Cryptographic Accumulators
Revocation of verifiable credentials (VCs) in the DPP system can follow one of two primary models – a status list or an accumulator. The W3C Status List 2021 is a standard that uses a published bitstring or list to indicate which credentials are valid or revoked [docs.truvera.io](https://docs.truvera.io). In practice, an issuer maintains a JSON document (or on-chain data) with an array of bits, where each index corresponds to a credential (e.g. based on a nonce or index in the credential ID). Revoking a credential means flipping the corresponding bit to “1” (revoked). This approach is simple and widely interoperable [docs.truvera.io](https://docs.truvera.io) – anyone can fetch the status list (or query the on-chain registry) to check if a given ID is present. It also allows batch updates: an issuer can revoke many credentials by publishing one updated list, which is efficient when using a blockchain (one transaction updates multiple statuses). However, status lists have downsides: they reveal which credential was revoked (since the index or ID is exposed) and the list size grows linearly with the number of credentials [docs.truvera.io](https://docs.truvera.io). This is where cryptographic accumulators come in as an alternative. An accumulator (like a Bloom filter or pairing-based accumulator) allows an issuer to cryptographically “accumulate” all valid credential IDs into one short digest. Instead of checking a list, a verifier checks a proof (witness) that a given credential’s ID is a member of the accumulator set (for non-revocation) or was removed. Systems like Hyperledger Indy and others use accumulators to enable privacy-preserving revocation: the holder of a credential can be given a witness at issuance, and if the credential is later revoked, the witness will fail to validate. The Truvera documentation explains that accumulators can prove non-revocation without disclosing the credential ID, whereas a status list requires telling the verifier the exact ID to look up [docs.truvera.io](https://docs.truvera.io) [docs.truvera.io](https://docs.truvera.io). Accumulators also keep constant size (the accumulator value and witness are fixed size no matter how many credentials) [docs.truvera.io](https://docs.truvera.io). The trade-off is that managing witnesses and performing accumulator math can be complex. In the DPP context, if privacy is a major concern (e.g. a company doesn’t want a public list of all product IDs it revoked), an accumulator-based revocation registry could be used. If simplicity and interoperability are priority (e.g. regulators want a straightforward list to audit), Status List 2021 may suffice. The platform could even support both: for example, Status List 2021 for general use (since it’s W3C standard and understood by many wallets) [docs.truvera.io](https://docs.truvera.io), and an accumulator for cases requiring anonymity (the OG tech or Truvera approach is to use an accumulator for “anonymous credentials” where even revocation checks don’t reveal which credential is being checked [docs.truvera.io](https://docs.truvera.io) [docs.truvera.io](https://docs.truvera.io)). It’s worth noting that using a status list on-chain can be efficient when batching, but if many single revocations are done, a specialized accumulator contract might incur less gas overall by not growing in storage per entry. The design decision may be to implement a VC Status Registry interface abstractly, and allow plugging in different backends (bit list or accumulator), based on credential type or issuer preference.

### Federated vs. Self-Sovereign Revocation Strategies
In a federated revocation model, a central authority or network maintains the revocation list for many issuers. An example is the EBSI Trusted Issuer Registry and Status Registry – a public ledger where any verifier can look up if a credential (by ID or hash) has been revoked by its issuer [ogtechnologies.co](https://ogtechnologies.co) [ogtechnologies.co](https://ogtechnologies.co). Federated models often come with governance: e.g., the network might require issuers to publish revocations to a common contract, ensuring consistency. This aids auditability – regulators can see all revoked credentials in one place. On the other hand, a self-sovereign revocation approach means each issuer is responsible for hosting or publishing revocation data for their own credentials. For instance, an issuer could include a URL in each credential pointing to a JSON status file they host; or they run their own smart contract keyed to their DID for revocations. This maximizes issuer control and can reduce dependency on a central service, but verifiers must know how to find and trust each issuer’s revocation mechanism (which can be addressed by standardizing the credentialStatus field in the VC to point to either a global or issuer-specific registry). The DPP platform leans towards a hybrid: use a shared ledger or registry for critical credentials (ensuring any stakeholder can quickly check status in one query) but also allow issuers to plug in their own revocation endpoints if they operate outside the common network. Self-sovereign methods align with the SSI principle that issuers control their data, but practically, for product passports that will be checked by many parties over a long supply chain, having a unified “phone book” of revocations is very helpful. One strategy is a federated network of status lists: e.g., the platform could maintain a distributed database where each issuer has a namespace, but queries can be aggregated through a single API. This is similar to how domain names work – decentralized responsibility but centralized resolution. In summary, federated revocation = easier lookup and governance (but possibly more overhead to maintain globally), self-sovereign revocation = more freedom (but requires robust discovery mechanisms). The DPP architecture document can outline how issuers integrate either approach, and verifiers always go through the standard interface (resolving the status given the info in the credential). By planning for both, the platform remains flexible as the ecosystem grows.

### Credential Lifecycle: Archival vs. Mutable Credentials
Product passports will often have a long lifespan, accumulating information from production to end-of-life. This raises the question: do we treat the VC as immutable snapshots that get superseded, or do we update one credential continuously? The W3C VC Data Model generally assumes credentials are tamper-evident and static once issued (you can always issue a new version, but you cannot change the signed data without invalidating the signature [github.com](https://github.com)). Therefore, one straightforward approach is delta issuance: whenever something about the product changes (ownership, repair, added carbon footprint data), the platform issues a new VC (or a new Verifiable Presentation containing the updated claims) and perhaps revokes the previous one. This provides an audit trail, as old versions can be archived. However, constantly re-issuing credentials could lead to many credentials per item, complicating tracking. An alternative concept is a composite or dynamic credential. For example, the DPP could be implemented as a collection of sub-credentials (each covering a data category like materials, repairs, etc.), all linked to the product’s DID. Updates then mean adding or revoking sub-credentials, rather than replacing the whole passport. Another approach is using linked data patches – essentially having one credential that contains a reference to a verifiable log of changes (the credential’s subject could be a product DID and the credential could include a hash of the current product data, which gets updated via a chain of linked transactions). The ZVEI DPP4.0 concept leans on an Asset Administration Shell (AAS) model where the DPP data is modular and can be signed in parts [zvei.org](https://zvei.org). That implies each “submodel” (like a module for sustainability data) could be signed by whoever is responsible (and updated independently). For long-term archival, the system should preserve historical records: every update event could be stored (on-chain as events or off-chain in an append-only log) with timestamps. This gives a chronological audit trail of the passport’s evolution – useful for compliance and investor due diligence (proving that certain data was present at a given time and not manipulated later). It might be wise to incorporate a versioning mechanism in credential IDs or metadata (e.g. a version number or effective date field). When a credential is superseded, the new one could carry a reference to the prior version (and the system could automatically mark the old as revoked or expired). This is analogous to a passport document in real life that gets renewed: the old one is invalid but kept on file. Lifecycle control also means deciding when a credential should expire. For instance, a digital passport might be set to expire after 10 years, or when the product is definitively recycled, whichever comes first – at which point no further updates are expected. Expiry combined with revocation ensures that credentials don’t live forever unused. In terms of mutable pathways, some industry proposals suggest the manufacturer could host an online data record that is always up-to-date, and the VC just authenticates the link to it. But that reintroduces centralization and reliance on the manufacturer’s database. A compromise is using the blockchain as a verifiable log of changes: e.g., each significant update (like “battery capacity after 3 years”) could be anchored as a small event, and the current credential could just reference the latest state hash (which a verifier can cross-check against the log). The platform documentation should clearly outline the expected lifecycle: e.g., “Upon product manufacture, a Version 1 DPP credential is issued. When the product is serviced or its status changes, a new credential (Version 2) is issued and the old one is set to expired in the status registry. All versions remain accessible for audit, but only the latest is considered active.” This ensures long-term archival (nothing is lost; investors or auditors can see the history) while making it easy for operational verifiers to get the current truth. The design must also consider storage: if each product ends up with 10 credentials over life, the system and wallets need to manage that gracefully (perhaps bundling them or showing only current unless deep audit is needed). By planning for delta updates and controlled mutability, the DPP platform can accommodate the dynamic nature of product data without sacrificing the verifiability and integrity guarantees of the VC model.

# Multilingual and Internationalized DPP Documents
Digital Product Passports will be used across borders, so the schema and data should accommodate multiple languages and locales. JSON-LD has a built-in support for language-tagged strings and even direction (ltr/rtl) markers. To internationalize text fields, define them in the context with a language mapping container. For example, a property like "productDescription" can be defined to allow a language map:
```json
"productDescription": { 
  "@id": "ex:description", 
  "@container": "@language" 
}
```
This means in the credential, productDescription can be an object with language keys: e.g., "productDescription": {"en": "High-efficiency solar panel", "de": "Hocheffizientes Solarmodul"}. Consumers will then pick the appropriate language. By indicating a default language in the context (using @language), you can also supply a default for strings without an explicit tag [stackoverflow.com](https://stackoverflow.com). There are no restrictions on language in JSON-LD data – the keys (property names) are IRIs or terms, and values can be in any language as long as appropriately tagged [stackoverflow.com](https://stackoverflow.com). We recommend providing translations for key human-readable fields (like product category names, compliance descriptions, etc.) within the DPP or via linked resources. For instance, if the DPP includes a code that represents a material type, that code could be resolvable to a concept that has rdfs:label in multiple languages in an ontology. Leverage linked data vocabularies that are already internationalized. Schema.org, for example, provides translations of terms in many languages. If suitable, mapping DPP fields to schema.org or other standard vocabularies can instantly grant multi-language support for those terms. Similarly, use controlled code lists (for product categories, materials, etc.) that publish labels in multiple languages. The DPP platform could incorporate an i18n module where all UI-facing strings or common taxonomy terms are stored with translations, ensuring that when a passport is displayed, it can show content in the user’s language. Another consideration is units and regional formats (though not strictly a language issue). Ensure that numeric data like dimensions or weight are accompanied by units (possibly SI units per regulation) so they’re unambiguous internationally. If necessary, the schema can allow multiple values with unit qualifiers (e.g., weight in kg and in local unit). However, typically the passport will standardize units to avoid confusion. For directionality, JSON-LD 1.1 supports @direction, though most product data text won’t require this unless including right-to-left scripts. In summary, to achieve multilingual DPPs: design the schema to accept language maps for text, use context @language to set defaults, pick vocabularies and code lists that come with translations, and possibly provide your own translation files for any static vocabulary. This ensures the same DPP can be understood by a German regulator or a French consumer just as well as by an English speaker, improving usability and compliance across markets. The underlying linked-data graph stays the same; it’s just enriched with language-tagged literals.
