# Smart Contract Deployment and Blockchain Anchoring

We anchor product integrity on the blockchain by storing cryptographic hashes of the product data. This provides a tamper-proof, time-stamped record of the product's state at any given point.

## Core Anchoring Strategy

The simplest method is to store a product's data hash on an EVM-compatible chain like Polygon PoS. The low transaction costs and fast finality make it ideal. A backend service, like a Firebase Cloud Function, is triggered when a passport is finalized. It serializes the product metadata (e.g., as JSON-LD), computes its `keccak256` hash, and calls a smart contract function to record it.

### Smart Contract Example: ProductRegistry

Below is a sample Solidity contract that records a product’s hash and emits an event.

```solidity
// SPDX-License-Identifier: MIT
pragma solidity ^0.8.0;

contract ProductRegistry {
    // Mapping from product ID to the stored hash (e.g. keccak256 of product metadata)
    mapping(uint256 => bytes32) public productHash;

    // Event emitted when a product hash is anchored on-chain
    event ProductAnchored(uint256 indexed productId, bytes32 hashValue);

    // Anchor the hash of arbitrary data for a given productId
    function anchorProductHash(uint256 productId, bytes calldata data) external {
        bytes32 h = keccak256(data);
        productHash[productId] = h;
        emit ProductAnchored(productId, h);
    }
}
```

This approach provides a solid foundation for data integrity verification.

## Advanced Concepts & EU Alignment

For deeper integration and alignment with EU frameworks like EBSI, we employ more advanced strategies including Verifiable Credentials (VCs), Decentralized Identifiers (DIDs), and NFTs.

### Cross-chain Validation and Modular Contracts
The DPP smart contracts should be blockchain-agnostic, enabling deployment on both EVM chains (Ethereum, Polygon, EBSI’s Ethereum-based network, etc.) and non-EVM ledgers (e.g. Hyperledger Fabric, Tezos, etc.). A modular architecture is recommended: separate the credential logic (issuance, revocation, verification) from chain-specific implementations. For example, a core library of credential verification (in Solidity or WebAssembly) could be reused, while adapters handle the transaction finality and identity method of each chain. The goal is that a Verifiable Credential issued on one network can be trusted on another, through either cross-chain proofs or common DID resolution. One strategy is to use decentralized identity as an abstraction layer: e.g., a product’s DID Document could contain multiple service endpoints or blockchain proofs (one per chain), and verifiers simply check whichever chain they have access to. For cross-chain verification, the platform might leverage interoperability protocols or bridges – for instance, using a notary smart contract on Chain B that accepts a cryptographic proof (perhaps a Merkle proof or simplified payment verification) of an anchor that originally lives on Chain A. Another approach is to anchor the same Merkle root on multiple chains for redundancy, so a credential can be verified against any of them. The EBSI network, for example, is EVM-compatible (built on Hyperledger Besu) [hub.ebsi.eu](https://hub.ebsi.eu), meaning Solidity contracts on EBSI can interoperate with Ethereum tooling, but the platform might also need to support non-EVM national blockchains or consortium ledgers. By designing an extendable anchor registry interface, the DPP can plug in new blockchain connectors as needed (e.g. one module for EVM chains, one for a Substrate-based chain, etc.), ensuring the core credential format remains consistent. This modular, cross-chain approach not only avoids vendor lock-in but also increases resilience – if one ledger is down or too expensive, critical passport validations can occur on an alternative network. The smart contracts thus act as one component in a broader distributed trust architecture, rather than a monolithic on-chain system.

### Solidity contract examples for VCs
A more advanced pattern uses issuer/verifier contracts for W3C Verifiable Credentials.

**Issuer Contract:**
```solidity
contract VCIssuer {
    mapping(address => bool) public issuers;
    mapping(address => mapping(string => string)) public creds;  // holder → (key → value)
    event Issued(address indexed issuer, address indexed holder, string key, string value);

    constructor() { issuers[msg.sender] = true; }
    modifier onlyIssuer() { require(issuers[msg.sender]); _; }

    function addIssuer(address newIssuer) external onlyIssuer {
        issuers[newIssuer] = true;
    }
    function issueCred(address holder, string calldata key, string calldata value) external onlyIssuer {
        creds[holder][key] = value;
        emit Issued(msg.sender, holder, key, value);
    }
}
```

**Verifier Contract:**
```solidity
contract VCVerifier {
    mapping(address => bool) public verifiers;
    event Verified(address indexed verifier, address indexed subject, string key, string value);

    constructor() { verifiers[msg.sender] = true; }
    modifier onlyVerifier() { require(verifiers[msg.sender]); _; }

    function addVerifier(address v) external onlyVerifier {
        verifiers[v] = true;
    }
    function verifyCred(address subject, string calldata key, string calldata expected) external onlyVerifier {
        // Note: This requires a way to reference the VCIssuerContract instance
        // string memory actual = VCIssuerContract(issuerAddress).creds(subject, key);
        // require(keccak256(bytes(actual)) == keccak256(bytes(expected)), "Verification failed");
        // emit Verified(msg.sender, subject, key, actual);
    }
}
```

### NFT/DID architecture
Each physical product can be represented by a unique NFT (ERC-721/1155) serving as its “digital twin.” The NFT metadata links to the DPP data, which is structured as a Verifiable Credential. The product’s globally unique identifier is a Decentralized Identifier (e.g., `did:web`, `did:ebsi`), linking the on-chain asset (NFT) to its off-chain data (VC).

### Verifiable Credential workflows
All DPPs are implemented as W3C Verifiable Credentials. The manufacturer (issuer) creates a JSON-LD credential, signs it with their private key, and makes it resolvable via the product's DID. This aligns with EU Digital Identity Wallet standards and ensures interoperability.

# Compliance Document Structuring

## JSON-LD and Semantic Web compliance
To maximize interoperability, all product passports are structured in JSON-LD (JSON for Linked Data). This allows every data point (e.g., a chemical ID) to be tied to a global, semantic definition (like a GS1 or UNECE vocabulary), making the data machine-readable and verifiable.

## Regulatory alignment (REACH, RoHS, ESPR, etc.)
The DPP schema is designed to directly map to the data requirements of major EU regulations. Fields for RoHS hazardous substances, REACH SVHCs, and ESPR-mandated data are all first-class citizens in the JSON-LD structure. This allows for automated compliance checking against regulatory thresholds.

## Credential signing workflow
Each DPP document is cryptographically signed to prevent tampering. We use JSON-LD Proofs, which are compatible with the EU's trust framework (e.g., eIDAS). In our system, an “EU-verifier” node, conforming to EBSI rules, can sign the final composite DPP as a Verifiable Credential and anchor its hash on the EBSI ledger. This provides a high degree of trust and legal non-repudiation.

# Smart Contract Optimization

## Efficient On-Chain Metadata Hash Storage
The DPP platform’s smart contracts should minimize on-chain operations by aggregating credential anchors. One approach is to use Merkle tree batching: hash each credential (or update) into a Merkle tree and only publish the root on-chain [cs.purdue.edu](https://cs.purdue.edu). This method allows thousands of credentials to share one anchor, drastically reducing gas costs (e.g. anchoring each VC individually might cost ~576k gas, versus ~78k gas for a single Merkle root that covers many credentials [cs.purdue.edu](https://cs.purdue.edu)). To preserve trust, off-chain proofs can be generated – a holder provides a Merkle inclusion proof that their credential was part of the anchored root, and verifiers can validate it against the on-chain root. Additionally, zero-knowledge proofs (ZKPs) can attest compliance without exposing data. Using ZK-SNARKs or similar, an issuer can prove a product meets certain standards (e.g. contains < X% of a restricted substance) without revealing the underlying proprietary data [circularise.com](https://circularise.com). This “proof-of-compliance” via ZKP balances transparency and IP protection, enabling verifiers to trust regulatory claims while sensitive product data remains confidential [circularise.com](https://circularise.com). The smart contracts would simply verify the ZK proof on-chain (or an off-chain service would do so and anchor a certificate), avoiding large data payloads and ensuring privacy.

## Gas Usage Minimization for Credential Anchoring
Every on-chain interaction (issuance, update, revocation) must be optimized for cost, especially if scaled to millions of products. By leveraging batched operations and compact data structures, the platform avoids redundant writes. For instance, instead of writing a new state for each credential update, the contract could store only a reference to a rolling Merkle root or use event logs for off-chain aggregation. Techniques like stateless proofs (where the chain stores a constant root and clients prove changes off-chain) help minimize gas. When an update or revocation occurs, multiple changes can be bundled into one transaction to amortize cost. The design should also consider layer-2 solutions or permissioned sidechains for heavy transaction throughput, anchoring periodically to a main chain for security. Within smart contracts, efficient coding (Solidity optimizations, using bitmaps or minimal storage slots for status flags, etc.) can further cut gas fees. One study showed that using a single Merkle root update for many credentials is orders of magnitude cheaper than individual writes [cs.purdue.edu](https://cs.purdue.edu). By carefully designing how credential states are recorded (or by not recording at all on-chain beyond a hash), the DPP platform ensures cost won’t balloon as more passports are issued and modified.

## Upgradability and Secure Design Practices
Digital Product Passport smart contracts must remain adaptable to evolving standards, so design for upgradability using proxy patterns. Smart contracts are immutable by default, but an upgradeable proxy architecture allows logic to be updated behind a persistent interface [docs.openzeppelin.com](https://docs.openzeppelin.com) [docs.openzeppelin.com](https://docs.openzeppelin.com). A common approach uses a thin proxy contract that delegates calls to an implementation contract which can be swapped as needed [docs.openzeppelin.com](https://docs.openzeppelin.com) [docs.openzeppelin.com](https://docs.openzeppelin.com). This proxy holds the state, while the implementation (business logic) can be upgraded under controlled conditions. Use OpenZeppelin’s Transparent or UUPS Proxy libraries for a battle-tested implementation of this pattern, and follow their guidelines (e.g. using an initializer function instead of constructor, and reserving storage slots for future use) for safe upgrades. Security is paramount – each contract update should undergo rigorous audits and testing. Leverage OpenZeppelin Contracts for standard functionality to reduce risk (e.g. use SafeMath, ERC standards implementations, etc. that are already audited). Integrate static analysis tools like Slither into the development pipeline; Slither can automatically detect common vulnerabilities (reentrancy, uninitialized storage, etc.) and even suggest gas optimizations [medium.com](https://medium.com) [medium.com](https://medium.com). It’s recommended to run Slither (and similar analyzers) regularly, and address all issues it flags before seeking an external audit [medium.com](https://medium.com) [medium.com](https://medium.com). Finally, perform manual code reviews and consider bug bounty programs. Upgradability must be paired with strong governance – use multi-signature wallets or timelocks for upgrade actions to prevent unauthorized or rushed upgrades. All these practices ensure the on-chain components of the DPP platform are efficient, secure, and maintainable over the long term.

# EBSI/SSI Node Deployment & Verifier Onboarding

## Becoming an EBSI-Compliant Issuer/Verifier
Onboarding as a trusted issuer or verifier in the European Blockchain Services Infrastructure (EBSI) ecosystem entails meeting legal, cryptographic, and procedural requirements. Legally, an organization must be recognized as a legitimate Trust Service Provider or accredited issuer under the EU’s framework. This starts with establishing a Decentralized Identifier (DID) for the legal entity (using the did:ebsi method or other allowed DID methods) and obtaining a Verifiable Authorisation to Onboard from the governing authority. In EBSI’s trust model, new issuers must receive a VerifiableAuthorisationToOnboard credential which allows them to anchor their DID Document on the EBSI DID Registry [hub.ebsi.eu](https://hub.ebsi.eu). This ensures the issuer’s DID is recorded on-chain and controlled by them (they prove ownership of the corresponding keys). Once onboarded, the entity may need an accreditation from a Root Trust Authority in the ecosystem. EBSI defines roles like Root Trusted Accreditation Organisations (Root TAOs), Trusted Accreditation Organisations (TAOs), and Trusted Issuers (TIs) [hub.ebsi.eu](https://hub.ebsi.eu) [hub.ebsi.eu](https://hub.ebsi.eu). In practice, this means an issuer (e.g., a certification body or a manufacturer) might first be accredited by a higher authority via a Verifiable Accreditation credential that specifies what types of credentials they can issue (for example, a ministry accredits a company to issue “Battery Compliance Certificates”). The issuer registers this in the Trusted Issuers Registry (TIR) on the blockchain [hub.ebsi.eu](https://hub.ebsi.eu) so that any verifier can cryptographically confirm the issuer’s legitimacy. Technically, the issuer must also prepare its infrastructure: they will need a secure wallet or HSM to manage signing keys, integration with the DPP platform’s APIs, and possibly run a node or use an API to write to the blockchain. For verifiers, onboarding focuses on trust consumption – they need to ensure they have the right root certificates and registry access. A verifier (e.g., customs officer’s app or a consumer’s wallet) will check that an issuer’s DID and credentials chain up to the European Trust Anchor. EBSI’s trust chain verification involves checking the issuer’s attestation from the European Trust Anchor (the EU Commission) and that the issuer appears in the Trusted Issuer Registry, as well as validating the credential’s signature and status [ogtechnologies.co](https://ogtechnologies.co) [ogtechnologies.co](https://ogtechnologies.co). Thus, any organization intending to verify DPP credentials may have to register for API access or software that can query EBSI’s registries (read access is public, but some verification software might require enrollment to get up-to-date trust lists). In summary, onboarding in an EU-aligned DPP network means proving who you are (through eIDAS-compliant identity), getting listed as trusted (through the blockchain-based registry and possibly legal agreements), and adhering to standards in how you issue or check credentials.

## TSP Integration and Qualified Signatures (eIDAS Compliance)
To align with EU regulations, the credentials in the DPP system should be signed with high-assurance keys. This typically involves using Trust Service Providers (TSPs) that issue Qualified Certificates for electronic seals or signatures under eIDAS (EU Regulation No. 910/2014). For an organization (like a manufacturer) issuing a product passport, this often means obtaining a Qualified Electronic Seal (QSeal) certificate – a digital certificate proving the organization’s identity, issued by an EU-authorized CA. The signing keys corresponding to this certificate are usually stored in a Hardware Security Module or provided via a Remote Signing Service by the TSP [ogtechnologies.co](https://ogtechnologies.co). The DPP platform can integrate with such a service so that every Verifiable Credential it issues is not only signed with a DID key, but also countersigned or backed by a qualified seal. This double-signing approach provides legal assurance: a Qualified Electronic Seal attached to a VC gives it the same legal status as a paper document stamped by the company [ogtechnologies.co](https://ogtechnologies.co). For issuers, the onboarding includes setting up this capability – e.g. using the InfoCert Remote QSeal Service or similar, which is explicitly designed to sign Verifiable Credentials with qualified certificates [impulse-h2020.eu](https://impulse-h2020.eu). Cryptographically, the platform might use the JSON Web Signature (JWS) or JSON-LD Signature format that can carry X.509 certificate data. The verifier software, in turn, will validate not just the VC’s integrity but also the certificate’s validity (checking it against EU Trusted Lists to ensure it’s a qualified certificate). This ensures that credentials are eIDAS compliant, meaning a verifier in any member state can trust the origin of the credential even if they don’t directly know the issuer – the signature itself is legally recognized. In practice, aligning with eIDAS may also require certain processes (the issuer might need to use a Qualified Signature Creation Device, and follow audit trails for when signatures are applied [entrust.com](https://entrust.com)). The DPP platform should enforce that only keys from a configured TSP are used for signing, and likely provide audit logs of credential issuance for compliance. By integrating trust services in this manner, the platform achieves a high level of assurance: credentials can stand up to regulatory scrutiny, carrying the presumption of authenticity and integrity across borders.

## Role of EUDI Wallets and OIDC4VC in Distribution
As Europe rolls out the European Digital Identity (EUDI) Wallet, the DPP platform is designed to interoperate with it for seamless credential distribution and inspection. In practical terms, this means using the OpenID Connect for Verifiable Credentials protocols (OIDC4VC) to issue and verify passports with user consent. For example, when a product is sold or a batch is manufactured, the issuer (or retailer) could offer the product’s passport to the buyer’s EUDI Wallet. The flow would work as follows: the wallet (acting as an OpenID client) receives a credential offer – perhaps through scanning a QR code on the product or an in-app notification. That QR code might encode an OpenID4VCI issuance initiation URL which the wallet uses to start the retrieval. The user would authenticate (possibly scanning a code or using their ID) and then the DPP platform’s issuer endpoint would send the Verifiable Credential into their wallet via a secure API [docs.igrant.io](https://docs.igrant.io) [docs.igrant.io](https://docs.igrant.io). All of this leverages standardized endpoints: an authorization server to get user consent, a Credential Issuance Endpoint to deliver the VC, etc., as defined by the OpenID4VCI spec. For verification, OIDC4VC’s counterpart (OpenID4VP) can be used. Suppose a recycling center or customs officer wants to verify a product’s passport. Using their app (verifier), they can present an OpenID4VP request (often via QR or NFC tap), which prompts the holder’s EUDI Wallet to generate a Verifiable Presentation of the passport credential. The presentation can be filtered (selective disclosure if needed, or combined with other credentials) and is sent back to the verifier’s system securely [docs.igrant.io](https://docs.igrant.io) [docs.igrant.io](https://docs.igrant.io). This flow ensures that the holder is in control and consents to sharing the data. In the background, the verifier’s system will validate the VP: checking the signatures (including that eIDAS seal), checking the issuer’s trust status (via EBSI TIR or similar), and the revocation status. The EUDI Wallet standards also emphasize privacy, so features like Selective Disclosure JWT (SD-JWT) or ZKP-based VPs may be in use [docs.igrant.io](https://docs.igrant.io) – e.g. a consumer can prove a product is compliant without revealing the entire passport. For the DPP platform, supporting EUDI wallets means implementing these endpoints and protocols, ensuring compliance with the Architecture Reference Framework (ARF) of the EU digital identity system [docs.igrant.io](https://docs.igrant.io). Additionally, the platform should handle OIDC credential offers – for instance, when a user scans a product’s QR, it might direct them to a web URL where they can authenticate and fetch the credential into their wallet. In summary, the DPP is not a standalone portal – it will plug into the broader wallet ecosystem, using OIDC4VC to push credentials to authorized wallets and OIDC4VP to let those credentials be verified by relying parties. This user-centric approach aligns with upcoming eIDAS 2.0 regulations and makes DPPs easily accessible: a battery passport or textile passport can live in the owner’s smartphone wallet, ready to be shown to a reseller, recycler, or inspector on demand.

# JSON-LD Schema Versioning & Credential Revocation

## Evolving JSON-LD Schemas with Backward Compatibility
As Digital Product Passport schemas evolve (new data fields, updated vocabularies, etc.), it’s critical to maintain backward compatibility so that older passports remain interpretable. A best practice is to version your JSON-LD contexts deliberately. For minor, additive changes, you can often update the existing context with new term definitions (JSON-LD allows adding terms without breaking older ones). For major changes, consider creating a new context URL (for example, dpp-schema-v2.jsonld) while still supporting the old one in verifiers. JSON-LD 1.1 introduced features like the @version tag and @protected to help manage context changes. Setting "@version": 1.1 in your context and marking terms as @protected prevents accidental override of important terms in extended contexts [w3c.github.io](https://w3c.github.io) [w3c.github.io](https://w3c.github.io). This means if a new context redefines a term that was marked protected, JSON-LD processors will throw an error – a useful safeguard to avoid silently altering the meaning of data. Use this to lock in critical terms (e.g., productID, manufacturer) so that any schema evolution doesn’t reinterpret them. For introducing new fields, design the schema such that unknown terms can be ignored by older software (the JSON-LD data model inherently allows ignoring undefined terms). For example, if version 1 of DPP schema has fields A, B, C and version 2 adds field D, an old client will just skip D. Never remove or repurpose fields in a non-backward-compatible way; instead deprecate gradually. If a field must be retired, you might keep it in the context but note it as deprecated (perhaps with an @deprecated annotation in documentation) and stop using it in new credentials, but maintain it for old ones. Another strategy is to namescape your terms by version, e.g., dpp:weight vs dpp_v2:weight if the definition changes meaning, though this can complicate consumers. Prefer to extend rather than redefine. Maintaining multiple context files is also an option: e.g., a base context for core terms and extension contexts per version or per industry. This modular approach allows combining contexts so that, for instance, a verifier might load both the base context and a context for “v2025 additions” to understand all terms. All JSON-LD contexts and schema definitions should be hosted at stable URLs, and older ones should remain accessible indefinitely. Cool URIs don’t change – once you publish a context at a URL, keep it live (or redirect appropriately) [w3c.github.io](https://w3c.github.io) [w3c.github.io](https://w3c.github.io). This ensures that even years later, a verifying party or a developer tool can retrieve the context and interpret an old credential. It’s wise to use a version identifier in the file name or path (e.g., /schemas/dpp/2024/context.jsonld for the 2024 version) so you can publish updates separately. In summary, plan a versioning policy from the start: support old schemas for their lifespan, publish clear changelogs for schema updates, and use JSON-LD features (@context, @version, @protected) to manage extensions safely.

## Credential Revocation Strategies
Managing the status and revocation of Verifiable Credentials is essential, especially for compliance documents like DPPs which might be updated or invalidated over time. The platform should implement a Credential Status mechanism as per W3C Verifiable Credentials recommendations. A common approach is the Status List 2021 (also known as bitstring status list) method: each credential carries a credentialStatus field pointing to a URL (or DID URL) that contains the status information. For high-volume scenarios, a single status list (a bit array where each bit corresponds to a credential) is efficient. EBSI, for example, endorses this approach – issuers maintain a credential status list and assign each credential a bit position in that list [hub.ebsi.eu](https://hub.ebsi.eu). The credentialStatus object in the DPP VC would include an identifier for the status list and an index (bit position). A 0 bit means the credential is valid, a 1 means revoked (or suspended) [hub.ebsi.eu](https://hub.ebsi.eu). When an issuer needs to revoke a DPP (say the product was recalled or data was found erroneous), they update their status list by flipping the corresponding bit. Verifiers can fetch this list (or a segment of it) and check the bit. Because the list can be hosted behind a privacy-preserving status proxy (as EBSI does) [hub.ebsi.eu](https://hub.ebsi.eu), verifiers don’t necessarily learn which credential is being checked – they just query by index. The DPP platform should provide tooling to manage these status lists or registries. This could mean hosting a simple API endpoint that returns a status list (which might be a compressed bitset or a JSON with indices of revoked credentials). Issuers might also choose to use an on-chain revocation registry (for instance, writing a transaction to a smart contract to mark a credential ID revoked). On-chain registries offer transparency and immutability, but have cost and privacy implications. Status lists (off-chain) are more scalable – a single bitlist can indicate status for thousands of credentials in one small file, and it can be signed by the issuer for authenticity. Another method is Revocation by reference: the credential could include an ID that gets published to a revocation registry if revoked. For example, some DID methods use a revocation registry entry or a Blockchain transaction to signal revocation. The trade-off is that checking revocation then requires consulting that blockchain or registry. For the EU’s purposes, the status list (with a proxy) approach is preferred to minimize correlation (the verifier doesn’t necessarily know which credential is being checked, since the proxy can handle requests in a way that hides the exact index being queried) [hub.ebsi.eu](https://hub.ebsi.eu). Additionally, expiration is a simple but important mechanism: every Verifiable Credential can have an expirationDate. The DPP credentials should have a reasonable validity period set (depending on the product type and regulations). For instance, a battery passport might be valid for the life of the battery or until a certain compliance deadline. Once past expiration, a credential is considered invalid by default. Expiration is not revocation per se, but it ensures credentials aren’t used beyond their intended timeframe. It’s straightforward for verifiers to check a timestamp. The platform should support revocation updates and notifications. If a DPP is revoked, ideally affected parties (e.g., the manufacturer, maybe the product owner) can be notified. One way is to leverage the DID Comm or webhook subscriptions to let certain participants know when a status changes. Internally, maintain an audit log of revocations: who revoked, when, and why – this is crucial for compliance traceability. In summary, implement a robust VC status system: likely by using W3C Status List 2021 spec (bit arrays) [hub.ebsi.eu](https://hub.ebsi.eu), hosted via a secure API; ensure every credential includes a status pointer; and support credential expiration and possibly on-chain registry integration for an extra layer of trust. This ensures verifiers can reliably detect invalidated passports (preventing fraud or misuse of revoked credentials) and that issuers have the tools to manage the lifecycle of credentials after issuance.

# Multilingual and Internationalized DPP Documents
Digital Product Passports will be used across borders, so the schema and data should accommodate multiple languages and locales. JSON-LD has a built-in support for language-tagged strings and even direction (ltr/rtl) markers. To internationalize text fields, define them in the context with a language mapping container. For example, a property like "productDescription" can be defined to allow a language map:
```json
"productDescription": { 
  "@id": "ex:description", 
  "@container": "@language" 
}
```
This means in the credential, productDescription can be an object with language keys: e.g., "productDescription": {"en": "High-efficiency solar panel", "de": "Hocheffizientes Solarmodul"}. Consumers will then pick the appropriate language. By indicating a default language in the context (using @language), you can also supply a default for strings without an explicit tag [stackoverflow.com](https://stackoverflow.com). There are no restrictions on language in JSON-LD data – the keys (property names) are IRIs or terms, and values can be in any language as long as appropriately tagged [stackoverflow.com](https://stackoverflow.com). We recommend providing translations for key human-readable fields (like product category names, compliance descriptions, etc.) within the DPP or via linked resources. For instance, if the DPP includes a code that represents a material type, that code could be resolvable to a concept that has rdfs:label in multiple languages in an ontology. Leverage linked data vocabularies that are already internationalized. Schema.org, for example, provides translations of terms in many languages. If suitable, mapping DPP fields to schema.org or other standard vocabularies can instantly grant multi-language support for those terms. Similarly, use controlled code lists (for product categories, materials, etc.) that publish labels in multiple languages. The DPP platform could incorporate an i18n module where all UI-facing strings or common taxonomy terms are stored with translations, ensuring that when a passport is displayed, it can show content in the user’s language. Another consideration is units and regional formats (though not strictly a language issue). Ensure that numeric data like dimensions or weight are accompanied by units (possibly SI units per regulation) so they’re unambiguous internationally. If necessary, the schema can allow multiple values with unit qualifiers (e.g., weight in kg and in local unit). However, typically the passport will standardize units to avoid confusion. For directionality, JSON-LD 1.1 supports @direction, though most product data text won’t require this unless including right-to-left scripts. In summary, to achieve multilingual DPPs: design the schema to accept language maps for text, use context @language to set defaults, pick vocabularies and code lists that come with translations, and possibly provide your own translation files for any static vocabulary. This ensures the same DPP can be understood by a German regulator or a French consumer just as well as by an English speaker, improving usability and compliance across markets. The underlying linked-data graph stays the same; it’s just enriched with language-tagged literals.
