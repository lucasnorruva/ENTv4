# Smart Contract Deployment and Blockchain Anchoring

We anchor product integrity on the blockchain by storing cryptographic hashes of the product data. This provides a tamper-proof, time-stamped record of the product's state at any given point.

## Core Anchoring Strategy

The simplest method is to store a product's data hash on an EVM-compatible chain like Polygon PoS. The low transaction costs and fast finality make it ideal. A backend service, like a Firebase Cloud Function, is triggered when a passport is finalized. It serializes the product metadata (e.g., as JSON-LD), computes its `keccak256` hash, and calls a smart contract function to record it.

### Smart Contract Example: ProductRegistry

Below is a sample Solidity contract that records a product’s hash and emits an event.

```solidity
// SPDX-License-Identifier: MIT
pragma solidity ^0.8.0;

contract ProductRegistry {
    // Mapping from product ID to the stored hash (e.g. keccak256 of product metadata)
    mapping(uint256 => bytes32) public productHash;

    // Event emitted when a product hash is anchored on-chain
    event ProductAnchored(uint256 indexed productId, bytes32 hashValue);

    // Anchor the hash of arbitrary data for a given productId
    function anchorProductHash(uint256 productId, bytes calldata data) external {
        bytes32 h = keccak256(data);
        productHash[productId] = h;
        emit ProductAnchored(productId, h);
    }
}
```

This approach provides a solid foundation for data integrity verification.

## Advanced Concepts & EU Alignment

For deeper integration and alignment with EU frameworks like EBSI, we employ more advanced strategies including Verifiable Credentials (VCs), Decentralized Identifiers (DIDs), and NFTs.

### Cross-chain Validation and Modular Contracts
The DPP smart contracts should be blockchain-agnostic, enabling deployment on both EVM chains (Ethereum, Polygon, EBSI’s Ethereum-based network, etc.) and non-EVM ledgers (e.g. Hyperledger Fabric, Tezos, etc.). A modular architecture is recommended: separate the credential logic (issuance, revocation, verification) from chain-specific implementations. For example, a core library of credential verification (in Solidity or WebAssembly) could be reused, while adapters handle the transaction finality and identity method of each chain. The goal is that a Verifiable Credential issued on one network can be trusted on another, through either cross-chain proofs or common DID resolution. One strategy is to use decentralized identity as an abstraction layer: e.g., a product’s DID Document could contain multiple service endpoints or blockchain proofs (one per chain), and verifiers simply check whichever chain they have access to. For cross-chain verification, the platform might leverage interoperability protocols or bridges – for instance, using a notary smart contract on Chain B that accepts a cryptographic proof (perhaps a Merkle proof or simplified payment verification) of an anchor that originally lives on Chain A. Another approach is to anchor the same Merkle root on multiple chains for redundancy, so a credential can be verified against any of them. The EBSI network, for example, is EVM-compatible (built on Hyperledger Besu) [hub.ebsi.eu](https://hub.ebsi.eu), meaning Solidity contracts on EBSI can interoperate with Ethereum tooling, but the platform might also need to support non-EVM national blockchains or consortium ledgers. By designing an extendable anchor registry interface, the DPP can plug in new blockchain connectors as needed (e.g. one module for EVM chains, one for a Substrate-based chain, etc.), ensuring the core credential format remains consistent. This modular, cross-chain approach not only avoids vendor lock-in but also increases resilience – if one ledger is down or too expensive, critical passport validations can occur on an alternative network. The smart contracts thus act as one component in a broader distributed trust architecture, rather than a monolithic on-chain system.

### Solidity contract examples for VCs
A more advanced pattern uses issuer/verifier contracts for W3C Verifiable Credentials.

**Issuer Contract:**
```solidity
contract VCIssuer {
    mapping(address => bool) public issuers;
    mapping(address => mapping(string => string)) public creds;  // holder → (key → value)
    event Issued(address indexed issuer, address indexed holder, string key, string value);

    constructor() { issuers[msg.sender] = true; }
    modifier onlyIssuer() { require(issuers[msg.sender]); _; }

    function addIssuer(address newIssuer) external onlyIssuer {
        issuers[newIssuer] = true;
    }
    function issueCred(address holder, string calldata key, string calldata value) external onlyIssuer {
        creds[holder][key] = value;
        emit Issued(msg.sender, holder, key, value);
    }
}
```

**Verifier Contract:**
```solidity
contract VCVerifier {
    mapping(address => bool) public verifiers;
    event Verified(address indexed verifier, address indexed subject, string key, string value);

    constructor() { verifiers[msg.sender] = true; }
    modifier onlyVerifier() { require(verifiers[msg.sender]); _; }

    function addVerifier(address v) external onlyVerifier {
        verifiers[v] = true;
    }
    function verifyCred(address subject, string calldata key, string calldata expected) external onlyVerifier {
        // Note: This requires a way to reference the VCIssuerContract instance
        // string memory actual = VCIssuerContract(issuerAddress).creds(subject, key);
        // require(keccak256(bytes(actual)) == keccak256(bytes(expected)), "Verification failed");
        // emit Verified(msg.sender, subject, key, actual);
    }
}
```

### NFT/DID architecture
Each physical product can be represented by a unique NFT (ERC-721/1155) serving as its “digital twin.” The NFT metadata links to the DPP data, which is structured as a Verifiable Credential. The product’s globally unique identifier is a Decentralized Identifier (e.g., `did:web`, `did:ebsi`), linking the on-chain asset (NFT) to its off-chain data (VC).

### Verifiable Credential workflows
All DPPs are implemented as W3C Verifiable Credentials. The manufacturer (issuer) creates a JSON-LD credential, signs it with their private key, and makes it resolvable via the product's DID. This aligns with EU Digital Identity Wallet standards and ensures interoperability.

# Compliance Document Structuring

## JSON-LD and Semantic Web compliance
To maximize interoperability, all product passports are structured in JSON-LD (JSON for Linked Data). This allows every data point (e.g., a chemical ID) to be tied to a global, semantic definition (like a GS1 or UNECE vocabulary), making the data machine-readable and verifiable.

## Regulatory alignment (REACH, RoHS, ESPR, etc.)
The DPP schema is designed to directly map to the data requirements of major EU regulations. Fields for RoHS hazardous substances, REACH SVHCs, and ESPR-mandated data are all first-class citizens in the JSON-LD structure. This allows for automated compliance checking against regulatory thresholds.

## Credential signing workflow
Each DPP document is cryptographically signed to prevent tampering. We use JSON-LD Proofs, which are compatible with the EU's trust framework (e.g., eIDAS). In our system, an “EU-verifier” node, conforming to EBSI rules, can sign the final composite DPP as a Verifiable Credential and anchor its hash on the EBSI ledger. This provides a high degree of trust and legal non-repudiation.

# Smart Contract Optimization

## Efficient On-Chain Metadata Hash Storage
The DPP platform’s smart contracts should minimize on-chain operations by aggregating credential anchors. One approach is to use Merkle tree batching: hash each credential (or update) into a Merkle tree and only publish the root on-chain [cs.purdue.edu](https://cs.purdue.edu). This method allows thousands of credentials to share one anchor, drastically reducing gas costs (e.g. anchoring each VC individually might cost ~576k gas, versus ~78k gas for a single Merkle root that covers many credentials [cs.purdue.edu](https://cs.purdue.edu)). To preserve trust, off-chain proofs can be generated – a holder provides a Merkle inclusion proof that their credential was part of the anchored root, and verifiers can validate it against the on-chain root. Additionally, zero-knowledge proofs (ZKPs) can attest compliance without exposing data. Using ZK-SNARKs or similar, an issuer can prove a product meets certain standards (e.g. contains < X% of a restricted substance) without revealing the underlying proprietary data [circularise.com](https://circularise.com). This “proof-of-compliance” via ZKP balances transparency and IP protection, enabling verifiers to trust regulatory claims while sensitive product data remains confidential [circularise.com](https://circularise.com). The smart contracts would simply verify the ZK proof on-chain (or an off-chain service would do so and anchor a certificate), avoiding large data payloads and ensuring privacy.

## Gas Usage Minimization for Credential Anchoring
Every on-chain interaction (issuance, update, revocation) must be optimized for cost, especially if scaled to millions of products. By leveraging batched operations and compact data structures, the platform avoids redundant writes. For instance, instead of writing a new state for each credential update, the contract could store only a reference to a rolling Merkle root or use event logs for off-chain aggregation. Techniques like stateless proofs (where the chain stores a constant root and clients prove changes off-chain) help minimize gas. When an update or revocation occurs, multiple changes can be bundled into one transaction to amortize cost. The design should also consider layer-2 solutions or permissioned sidechains for heavy transaction throughput, anchoring periodically to a main chain for security. Within smart contracts, efficient coding (Solidity optimizations, using bitmaps or minimal storage slots for status flags, etc.) can further cut gas fees. One study showed that using a single Merkle root update for many credentials is orders of magnitude cheaper than individual writes [cs.purdue.edu](https://cs.purdue.edu). By carefully designing how credential states are recorded (or by not recording at all on-chain beyond a hash), the DPP platform ensures cost won’t balloon as more passports are issued and modified.

## Upgradability and Secure Design Practices
Digital Product Passport smart contracts must remain adaptable to evolving standards, so design for upgradability using proxy patterns. Smart contracts are immutable by default, but an upgradeable proxy architecture allows logic to be updated behind a persistent interface [docs.openzeppelin.com](https://docs.openzeppelin.com) [docs.openzeppelin.com](https://docs.openzeppelin.com). A common approach uses a thin proxy contract that delegates calls to an implementation contract which can be swapped as needed [docs.openzeppelin.com](https://docs.openzeppelin.com) [docs.openzeppelin.com](https://docs.openzeppelin.com). This proxy holds the state, while the implementation (business logic) can be upgraded under controlled conditions. Use OpenZeppelin’s Transparent or UUPS Proxy libraries for a battle-tested implementation of this pattern, and follow their guidelines (e.g. using an initializer function instead of constructor, and reserving storage slots for future use) for safe upgrades. Security is paramount – each contract update should undergo rigorous audits and testing. Leverage OpenZeppelin Contracts for standard functionality to reduce risk (e.g. use SafeMath, ERC standards implementations, etc. that are already audited). Integrate static analysis tools like Slither into the development pipeline; Slither can automatically detect common vulnerabilities (reentrancy, uninitialized storage, etc.) and even suggest gas optimizations [medium.com](https://medium.com) [medium.com](https://medium.com). It’s recommended to run Slither (and similar analyzers) regularly, and address all issues it flags before seeking an external audit [medium.com](https://medium.com) [medium.com](https://medium.com). Finally, perform manual code reviews and consider bug bounty programs. Upgradability must be paired with strong governance – use multi-signature wallets or timelocks for upgrade actions to prevent unauthorized or rushed upgrades. All these practices ensure the on-chain components of the DPP platform are efficient, secure, and maintainable over the long term.

# EBSI/SSI Node Deployment & Verifier Onboarding

## Becoming a Trusted Credential Issuer in EBSI
Joining the European Blockchain Services Infrastructure (EBSI) ecosystem as a credential issuer (Trusted Issuer) involves a multi-step onboarding process. First, an organization needs to set up an organization digital wallet that is EBSI-compliant, with the capability to issue and manage credentials [hub.ebsi.eu](https://hub.ebsi.eu). In this step, the org generates a Decentralized Identifier (DID) – using the EBSI DID method for legal entities (did:ebsi) – and produces a DID document containing the required public keys (notably an ES256K key as mandated by EBSI) [hub.ebsi.eu](https://hub.ebsi.eu). Next, the organization must obtain a Verifiable Authorisation to Onboard from a relevant Trust Accreditation Organization (TAO) or Root TAO [hub.ebsi.eu](https://hub.ebsi.eu) [hub.ebsi.eu](https://hub.ebsi.eu). This is essentially a credential issued by the governing authority that authorizes the new issuer to register on the EBSI network. The steps include:

1.  **Pre-authorization setup**: Ensure your organization meets preliminary requirements – having the wallet and DID in place, and being accredited or approved by whatever onboarding program is in place [hub.ebsi.eu](https://hub.ebsi.eu).
2.  **Request Verifiable Authorization**: Submit a request (often through the EBSI portal or via the TAO) to get the “authorization to onboard” credential [hub.ebsi.eu](https://hub.ebsi.eu). This likely involves providing evidence of your organization’s identity and legal status to the TAO.
3.  **Register DID on EBSI**: Once the authorization is received, use it to insert your DID document into the EBSI DID Registry via the provided API (JSON-RPC call insertDidDocument) [hub.ebsi.eu](https://hub.ebsi.eu). This on-chain registration makes your DID official and discoverable on the EBSI network.
4.  **Enroll in the Trusted Issuers Registry**: After DID registration, the organization must register as a Trusted Issuer in EBSI’s Trusted Issuers Registry (TIR). This may involve obtaining an access token (tir_invite) via EBSI’s Authorization API, proving control of the DID (e.g., by signing a challenge) [hub.ebsi.eu](https://hub.ebsi.eu). Once invited, the issuer’s profile (DID, accreditation info, etc.) is added to the registry of trusted credential issuers.
5.  **Credential Issuance Setup**: The issuer can then configure its systems to issue Verifiable Credentials (VCs) under EBSI standards. This includes hosting a VC Status List service if revocable credentials are to be issued, and using EBSI-compliant schemas for the credential data. The issuer should follow EBSI’s VC data models for specific credential types (e.g., education diploma, product passport, etc.).
6.  **Sign Credentials with EBSI-compliant Signatures**: When issuing, the credential must be signed in a format acceptable to EBSI. Specifically, EU-regulation compliant signatures are required: issuers in the European Single Market must use JAdES (JSON Advanced Electronic Signature), which is an ETSI standard augmenting JWS with eIDAS-compliant properties [hub.ebsi.eu](https://hub.ebsi.eu) [hub.ebsi.eu](https://hub.ebsi.eu). The issuer’s signing keys should be backed by certificates from an eIDAS trust service. The result is a JWT-based credential (JWT-VC) with an advanced electronic seal/signature, giving legal weight and cross-border trust.

By following these steps, an organization becomes a recognized Trusted Issuer on EBSI, able to issue Digital Product Passports or other credentials that member state wallets and verifiers will trust.

## Running a Trusted Verifier Node – Infrastructure Requirements
Organizations that frequently verify credentials (e.g., compliance authorities or large industry players) may run their own EBSI node as a verifier. EBSI’s network consists of blockchain nodes that maintain the ledger of DIDs and possibly credential status registries. Hardware and network requirements for an EBSI node are non-trivial, aimed at enterprise-grade deployment. Each node needs a dedicated server or virtual machine with a minimum of about 4–8 vCPUs, 32–64 GB of RAM, and fast storage (e.g. ≥256 GB SSD for data) [hub.ebsi.eu](https://hub.ebsi.eu). For example, EBSI’s guidance suggests ~4 vCPUs and 32 GB RAM for testing environments, and 8 vCPUs with 64 GB RAM for production nodes [hub.ebsi.eu](https://hub.ebsi.eu). Disk requirements scale from ~256 GB in pilot networks up to 500 GB or more in production for ledger data [hub.ebsi.eu](https://hub.ebsi.eu). Each node must have reliable network connectivity with at least a 100 Mbps internet link and fixed public IPv4 address(es) [hub.ebsi.eu](https://hub.ebsi.eu). Low latency (<100ms) to other European nodes is expected for proper synchronization [hub.ebsi.eu](https://hub.ebsi.eu). Node operators are typically required to host the node within the EU/EEA (to comply with data jurisdiction rules) and in a secure environment (data center or cloud in Europe) [hub.ebsi.eu](https://hub.ebsi.eu) [hub.ebsi.eu](https://hub.ebsi.eu). The software stack for an EBSI node is provided as a pre-packaged virtual appliance (e.g., as VMware OVA or a QEMU image) containing the blockchain client (EBSI uses a variant of Ethereum, Hyperledger Besu) and the auxiliary services [hub.ebsi.eu](https://hub.ebsi.eu). The node images come pre-configured to join the EBSI network, exposing APIs (such as a REST/JSON API for credential verification, DID resolution, etc.) on specified endpoints. Operational requirements include maintaining a high-availability setup (especially if running a validator node). There are Service Level Agreements – validator nodes have stricter uptime and response time obligations than regular read-only nodes [ec.europa.eu](https://ec.europa.eu). Security-wise, the node should sit behind a firewall, with only necessary ports open, and implement DDoS protection [hub.ebsi.eu](https://hub.ebsi.eu) [hub.ebsi.eu](https://hub.ebsi.eu). For production usage, EBSI mandates compliance with standards like ISO 27001 for information security management by the node operator [hub.ebsi.eu](https://hub.ebsi.eu). In summary, a verifier node deployment calls for enterprise IT infrastructure: robust hardware, secure hosting, and ongoing monitoring of CPU, memory, and bandwidth to scale resources as needed [hub.ebsi.eu](https://hub.ebsi.eu). Alternatively, if an organization does not want to host an entire node, they can use the APIs of existing nodes or third-party EBSI services to perform verification – but running a node gives direct, sovereign access to the trusted ledger and can improve verification speed and trust (since you rely on your own infrastructure).

## Integration with eIDAS Trust Services and EU Wallet Requirements
To issue verifiable credentials that will be accepted by the upcoming European Digital Identity Wallet, organizations must integrate with eIDAS-qualified trust service providers. Under eIDAS, a Qualified Trust Service Provider (QTSP) can issue qualified certificates for electronic seals or signatures. A DPP issuer should procure a Qualified Electronic Seal Certificate (QSealC) or a Qualified Certificate for Electronic Signatures, which it will use to sign the credentials. By doing so, the credentials can become Qualified Electronic Attestations of Attributes (QEAA) under the EU wallet framework [ec.europa.eu](https://ec.europa.eu). QEAAs are credentials whose issuers underwent extra trust steps and whose signatures are qualified – they enjoy the same legal validity as paper documents [ec.europa.eu](https://ec.europa.eu). For instance, a Digital Product Passport signed with a qualified certificate is legally robust and can be automatically trusted by wallet apps and regulators, as its issuer’s certificate chains to the EU Trusted List of QTSPs. In practice, integration with eIDAS trust services means the DPP platform should support JAdES digital signatures (as mentioned earlier) and the management of signing keys/certificates. Issuers might use an HSM (Hardware Security Module) or a cloud signing service provided by a QTSP to produce the JAdES signatures. EBSI’s guidelines explicitly state that in the European Single Market, JAdES must be used for VC signatures to comply with the eIDAS Regulation [hub.ebsi.eu](https://hub.ebsi.eu). These JAdES signatures embed the issuer’s certificate and meet ETSI standards for advanced electronic seals [hub.ebsi.eu](https://hub.ebsi.eu). The platform should also handle certificate renewal and verification: verifiers will need to validate the signature on a credential and ensure the certificate was valid and not revoked, which involves checking against the EU Trusted List (a public list of qualified certificates). Additionally, when issuing credentials for the EU wallet, certain wallet-specific specs must be adhered to (as per the forthcoming eIDAS 2.0 and EU Digital Identity Wallet architecture). This could include aligning with the European Wallet Architecture and Reference Framework for data formats, using the proper OIDC-4-VP/VC protocols for presenting credentials, and ensuring the credential schema matches the EU’s reference schemas for that attribute type. The DPP platform should be ready to plug into national ID wallet ecosystems – for example, by providing OIDC-compliant endpoints that wallets use to retrieve or verify a credential. In summary, eIDAS integration ensures that DPP credentials are not just technically verifiable, but legally and cross-jurisdictionally recognized. This builds trust with regulators and large enterprise customers, as the credentials carry legal weight. By using qualified signatures (JAdES) and trust list verification, the platform aligns with EU wallet requirements and becomes part of the official trust ecosystem, rather than a standalone solution [hub.ebsi.eu](https://hub.ebsi.eu) [ec.europa.eu](https://ec.europa.eu).

# JSON-LD Schema Versioning & Credential Revocation

## Evolving JSON-LD Schemas with Backward Compatibility
As Digital Product Passport schemas evolve (new data fields, updated vocabularies, etc.), it’s critical to maintain backward compatibility so that older passports remain interpretable. A best practice is to version your JSON-LD contexts deliberately. For minor, additive changes, you can often update the existing context with new term definitions (JSON-LD allows adding terms without breaking older ones). For major changes, consider creating a new context URL (for example, dpp-schema-v2.jsonld) while still supporting the old one in verifiers. JSON-LD 1.1 introduced features like the @version tag and @protected to help manage context changes. Setting "@version": 1.1 in your context and marking terms as @protected prevents accidental override of important terms in extended contexts [w3c.github.io](https://w3c.github.io) [w3c.github.io](https://w3c.github.io). This means if a new context redefines a term that was marked protected, JSON-LD processors will throw an error – a useful safeguard to avoid silently altering the meaning of data. Use this to lock in critical terms (e.g., productID, manufacturer) so that any schema evolution doesn’t reinterpret them. For introducing new fields, design the schema such that unknown terms can be ignored by older software (the JSON-LD data model inherently allows ignoring undefined terms). For example, if version 1 of DPP schema has fields A, B, C and version 2 adds field D, an old client will just skip D. Never remove or repurpose fields in a non-backward-compatible way; instead deprecate gradually. If a field must be retired, you might keep it in the context but note it as deprecated (perhaps with an @deprecated annotation in documentation) and stop using it in new credentials, but maintain it for old ones. Another strategy is to namescape your terms by version, e.g., dpp:weight vs dpp_v2:weight if the definition changes meaning, though this can complicate consumers. Prefer to extend rather than redefine. Maintaining multiple context files is also an option: e.g., a base context for core terms and extension contexts per version or per industry. This modular approach allows combining contexts so that, for instance, a verifier might load both the base context and a context for “v2025 additions” to understand all terms. All JSON-LD contexts and schema definitions should be hosted at stable URLs, and older ones should remain accessible indefinitely. Cool URIs don’t change – once you publish a context at a URL, keep it live (or redirect appropriately) [w3c.github.io](https://w3c.github.io) [w3c.github.io](https://w3c.github.io). This ensures that even years later, a verifying party or a developer tool can retrieve the context and interpret an old credential. It’s wise to use a version identifier in the file name or path (e.g., /schemas/dpp/2024/context.jsonld for the 2024 version) so you can publish updates separately. In summary, plan a versioning policy from the start: support old schemas for their lifespan, publish clear changelogs for schema updates, and use JSON-LD features (@context, @version, @protected) to manage extensions safely.

## Credential Revocation Strategies
Managing the status and revocation of Verifiable Credentials is essential, especially for compliance documents like DPPs which might be updated or invalidated over time. The platform should implement a Credential Status mechanism as per W3C Verifiable Credentials recommendations. A common approach is the Status List 2021 (also known as bitstring status list) method: each credential carries a credentialStatus field pointing to a URL (or DID URL) that contains the status information. For high-volume scenarios, a single status list (a bit array where each bit corresponds to a credential) is efficient. EBSI, for example, endorses this approach – issuers maintain a credential status list and assign each credential a bit position in that list [hub.ebsi.eu](https://hub.ebsi.eu). The credentialStatus object in the DPP VC would include an identifier for the status list and an index (bit position). A 0 bit means the credential is valid, a 1 means revoked (or suspended) [hub.ebsi.eu](https://hub.ebsi.eu). When an issuer needs to revoke a DPP (say the product was recalled or data was found erroneous), they update their status list by flipping the corresponding bit. Verifiers can fetch this list (or a segment of it) and check the bit. Because the list can be hosted behind a privacy-preserving status proxy (as EBSI does) [hub.ebsi.eu](https://hub.ebsi.eu), verifiers don’t necessarily learn which credential is being checked – they just query by index. The DPP platform should provide tooling to manage these status lists or registries. This could mean hosting a simple API endpoint that returns a status list (which might be a compressed bitset or a JSON with indices of revoked credentials). Issuers might also choose to use an on-chain revocation registry (for instance, writing a transaction to a smart contract to mark a credential ID revoked). On-chain registries offer transparency and immutability, but have cost and privacy implications. Status lists (off-chain) are more scalable – a single bitlist can indicate status for thousands of credentials in one small file, and it can be signed by the issuer for authenticity. Another method is Revocation by reference: the credential could include an ID that gets published to a revocation registry if revoked. For example, some DID methods use a revocation registry entry or a Blockchain transaction to signal revocation. The trade-off is that checking revocation then requires consulting that blockchain or registry. For the EU’s purposes, the status list (with a proxy) approach is preferred to minimize correlation (the verifier doesn’t necessarily know which credential is being checked, since the proxy can handle requests in a way that hides the exact index being queried) [hub.ebsi.eu](https://hub.ebsi.eu). Additionally, expiration is a simple but important mechanism: every Verifiable Credential can have an expirationDate. The DPP credentials should have a reasonable validity period set (depending on the product type and regulations). For instance, a battery passport might be valid for the life of the battery or until a certain compliance deadline. Once past expiration, a credential is considered invalid by default. Expiration is not revocation per se, but it ensures credentials aren’t used beyond their intended timeframe. It’s straightforward for verifiers to check a timestamp. The platform should support revocation updates and notifications. If a DPP is revoked, ideally affected parties (e.g., the manufacturer, maybe the product owner) can be notified. One way is to leverage the DID Comm or webhook subscriptions to let certain participants know when a status changes. Internally, maintain an audit log of revocations: who revoked, when, and why – this is crucial for compliance traceability. In summary, implement a robust VC status system: likely by using W3C Status List 2021 spec (bit arrays) [hub.ebsi.eu](https://hub.ebsi.eu), hosted via a secure API; ensure every credential includes a status pointer; and support credential expiration and possibly on-chain registry integration for an extra layer of trust. This ensures verifiers can reliably detect invalidated passports (preventing fraud or misuse of revoked credentials) and that issuers have the tools to manage the lifecycle of credentials after issuance.

# Multilingual and Internationalized DPP Documents
Digital Product Passports will be used across borders, so the schema and data should accommodate multiple languages and locales. JSON-LD has a built-in support for language-tagged strings and even direction (ltr/rtl) markers. To internationalize text fields, define them in the context with a language mapping container. For example, a property like "productDescription" can be defined to allow a language map:
```json
"productDescription": { 
  "@id": "ex:description", 
  "@container": "@language" 
}
```
This means in the credential, productDescription can be an object with language keys: e.g., "productDescription": {"en": "High-efficiency solar panel", "de": "Hocheffizientes Solarmodul"}. Consumers will then pick the appropriate language. By indicating a default language in the context (using @language), you can also supply a default for strings without an explicit tag [stackoverflow.com](https://stackoverflow.com). There are no restrictions on language in JSON-LD data – the keys (property names) are IRIs or terms, and values can be in any language as long as appropriately tagged [stackoverflow.com](https://stackoverflow.com). We recommend providing translations for key human-readable fields (like product category names, compliance descriptions, etc.) within the DPP or via linked resources. For instance, if the DPP includes a code that represents a material type, that code could be resolvable to a concept that has rdfs:label in multiple languages in an ontology. Leverage linked data vocabularies that are already internationalized. Schema.org, for example, provides translations of terms in many languages. If suitable, mapping DPP fields to schema.org or other standard vocabularies can instantly grant multi-language support for those terms. Similarly, use controlled code lists (for product categories, materials, etc.) that publish labels in multiple languages. The DPP platform could incorporate an i18n module where all UI-facing strings or common taxonomy terms are stored with translations, ensuring that when a passport is displayed, it can show content in the user’s language. Another consideration is units and regional formats (though not strictly a language issue). Ensure that numeric data like dimensions or weight are accompanied by units (possibly SI units per regulation) so they’re unambiguous internationally. If necessary, the schema can allow multiple values with unit qualifiers (e.g., weight in kg and in local unit). However, typically the passport will standardize units to avoid confusion. For directionality, JSON-LD 1.1 supports @direction, though most product data text won’t require this unless including right-to-left scripts. In summary, to achieve multilingual DPPs: design the schema to accept language maps for text, use context @language to set defaults, pick vocabularies and code lists that come with translations, and possibly provide your own translation files for any static vocabulary. This ensures the same DPP can be understood by a German regulator or a French consumer just as well as by an English speaker, improving usability and compliance across markets. The underlying linked-data graph stays the same; it’s just enriched with language-tagged literals.
